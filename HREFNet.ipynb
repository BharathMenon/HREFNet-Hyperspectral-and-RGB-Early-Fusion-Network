{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":75232,"databundleVersionId":8331367,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing of Hyperspectral Images","metadata":{}},{"cell_type":"code","source":"!pip install rasterio\n!pip install spectral\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:26:48.216760Z","iopub.execute_input":"2025-04-15T18:26:48.217244Z","iopub.status.idle":"2025-04-15T18:26:59.270005Z","shell.execute_reply.started":"2025-04-15T18:26:48.217217Z","shell.execute_reply":"2025-04-15T18:26:59.268945Z"}},"outputs":[{"name":"stdout","text":"Collecting rasterio\n  Downloading rasterio-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\nCollecting affine (from rasterio)\n  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (25.1.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2025.1.31)\nRequirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\nRequirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.26.4)\nRequirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from rasterio) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->rasterio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->rasterio) (2024.2.0)\nDownloading rasterio-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\nInstalling collected packages: affine, rasterio\nSuccessfully installed affine-2.4.0 rasterio-1.4.3\nCollecting spectral\n  Downloading spectral-0.24-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->spectral) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->spectral) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->spectral) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->spectral) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->spectral) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->spectral) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->spectral) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->spectral) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->spectral) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->spectral) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->spectral) (2024.2.0)\nDownloading spectral-0.24-py3-none-any.whl (249 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.0/249.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: spectral\nSuccessfully installed spectral-0.24\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rasterio\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\n# Define dataset paths\nraw_dataset_path = \"/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2024/archive\"  \nprocessed_dataset_path = \"/kaggle/working/processed_images\"\nos.makedirs(processed_dataset_path, exist_ok=True)\n\n# Standard shape for hyperspectral processing\nTARGET_SIZE = (128, 128)  # Resize to 128x128\nSELECTED_BANDS = 100  # Number of spectral bands to keep\n\n# Function to process a single hyperspectral image\ndef process_hyperspectral_image(image_path):\n    with rasterio.open(image_path) as img:\n        hyperspectral_data = img.read()  # Shape: (Bands, Height, Width)\n    \n    # Normalize each band separately\n    hyperspectral_data = hyperspectral_data.astype(np.float32)\n    for i in range(hyperspectral_data.shape[0]):  # Iterate over bands\n        min_val = hyperspectral_data[i].min()\n        max_val = hyperspectral_data[i].max()\n        if max_val > min_val:\n            hyperspectral_data[i] = (hyperspectral_data[i] - min_val) / (max_val - min_val)\n\n    # Perform band selection (Take first 50 bands or use PCA if needed)\n    if hyperspectral_data.shape[0] > SELECTED_BANDS:\n        hyperspectral_data = hyperspectral_data[:SELECTED_BANDS, :, :]\n\n    # Resize each band separately\n    resized_bands = np.zeros((SELECTED_BANDS, TARGET_SIZE[0], TARGET_SIZE[1]))\n    for i in range(SELECTED_BANDS):\n        resized_bands[i] = cv2.resize(hyperspectral_data[i], TARGET_SIZE, interpolation=cv2.INTER_CUBIC)\n    \n    return resized_bands  # Shape: (50, 128, 128)\n\n# Process entire dataset\nall_images = []\nall_labels = []\n\nfor category in os.listdir(os.path.join(raw_dataset_path, \"train\")):  # Using \"train\" folder as full dataset\n    category_path = os.path.join(raw_dataset_path, \"train\", category)\n    os.makedirs(os.path.join(processed_dataset_path, category), exist_ok=True)\n\n    for img_path in tqdm(glob(os.path.join(category_path, \"*.tif\")), desc=f\"Processing {category}\"):\n        processed_image = process_hyperspectral_image(img_path)\n        \n        # Save as .npy file\n        save_path = os.path.join(processed_dataset_path, category, os.path.basename(img_path).replace(\".tif\", \".npy\"))\n        np.save(save_path, processed_image)\n\n        all_images.append(save_path)\n        all_labels.append(category)\n\nprint(f\"✅ Processed {len(all_images)} images! Stored in {processed_dataset_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:26:59.271650Z","iopub.execute_input":"2025-04-15T18:26:59.272003Z","iopub.status.idle":"2025-04-15T18:27:33.851539Z","shell.execute_reply.started":"2025-04-15T18:26:59.271969Z","shell.execute_reply":"2025-04-15T18:27:33.850622Z"}},"outputs":[{"name":"stderr","text":"Processing Other: 100%|██████████| 200/200 [00:10<00:00, 18.94it/s]\nProcessing Health: 100%|██████████| 200/200 [00:10<00:00, 18.51it/s]\nProcessing Rust: 100%|██████████| 200/200 [00:11<00:00, 17.59it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Processed 600 images! Stored in /kaggle/working/processed_images\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Split dataset into train (80%), val (10%), and test (10%)\ntrain_imgs, temp_imgs, train_labels, temp_labels = train_test_split(all_images, all_labels, test_size=0.2, stratify=all_labels, random_state=42)\nval_imgs, test_imgs, val_labels, test_labels = train_test_split(temp_imgs, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)\n\n# Create new directories\nsplit_dataset_path = \"/kaggle/working/split_dataset\"\nfor split in [\"train\", \"val\", \"test\"]:\n    for category in os.listdir(processed_dataset_path):\n        os.makedirs(os.path.join(split_dataset_path, split, category), exist_ok=True)\n\n# Function to move files to split directories\nimport shutil\n\ndef move_files(image_paths, labels, split_name):\n    for img_path, label in zip(image_paths, labels):\n        dest_path = os.path.join(split_dataset_path, split_name, label, os.path.basename(img_path))\n        shutil.move(img_path, dest_path)\n\n# Move processed files to split directories\nmove_files(train_imgs, train_labels, \"train\")\nmove_files(val_imgs, val_labels, \"val\")\nmove_files(test_imgs, test_labels, \"test\")\n\nprint(f\"✅ Dataset split completed! Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test: {len(test_imgs)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:27:33.852791Z","iopub.execute_input":"2025-04-15T18:27:33.853229Z","iopub.status.idle":"2025-04-15T18:27:33.887792Z","shell.execute_reply.started":"2025-04-15T18:27:33.853206Z","shell.execute_reply":"2025-04-15T18:27:33.887170Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset split completed! Train: 480, Val: 60, Test: 60\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Making RGB Images","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\nimport shutil\n\n# Define source and destination directories for split data\nsplit_dataset_path = \"/kaggle/working/split_dataset\"\nrgb_split_dataset_path = \"/kaggle/working/rgb_split_dataset\"\nos.makedirs(rgb_split_dataset_path, exist_ok=True)\n\n# Create corresponding folder structure for RGB images\nfor split in [\"train\", \"val\", \"test\"]:\n    split_path = os.path.join(split_dataset_path, split)\n    rgb_split_path = os.path.join(rgb_split_dataset_path, split)\n    os.makedirs(rgb_split_path, exist_ok=True)\n    \n    # For every category folder in this split:\n    for category in os.listdir(split_path):\n        category_dir = os.path.join(split_path, category)\n        if os.path.isdir(category_dir):\n            rgb_category_dir = os.path.join(rgb_split_path, category)\n            os.makedirs(rgb_category_dir, exist_ok=True)\n\ndef hsi_to_rgb(hsi_image, r_idx=42, g_idx=15, b_idx=0):\n    \"\"\"\n    Convert hyperspectral image (shape: (channels, H, W)) to a pseudo-RGB image using band selection.\n    \n    The three chosen indices are arbitrary and should be tailored to your sensor.\n    \"\"\"\n    # Ensure hsi_image is a numpy array\n    if isinstance(hsi_image, (list, tuple)):\n        hsi_image = np.array(hsi_image)\n    \n    # hsi_image is assumed to be normalized to [0,1]\n    # Extract the three bands (shape: (H, W))\n    try:\n        red_band   = hsi_image[r_idx]\n        green_band = hsi_image[g_idx]\n        blue_band  = hsi_image[b_idx]\n    except IndexError:\n        raise ValueError(\"Chosen band indices are out of bounds for the image with shape {}\".format(hsi_image.shape))\n    \n    # Stack into RGB (H, W, 3)\n    rgb = np.stack([red_band, green_band, blue_band], axis=-1)\n    \n    # Optional: clip or re-normalize if needed\n    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8)  # Back to [0,1]\n    rgb = (rgb * 255).astype(np.uint8)  # Convert to 8-bit image\n    return rgb\n\n# Loop over the split directories and convert/save RGB images\nfor split in [\"train\", \"val\", \"test\"]:\n    split_path = os.path.join(split_dataset_path, split)\n    rgb_split_path = os.path.join(rgb_split_dataset_path, split)\n    for category in os.listdir(split_path):\n        category_dir = os.path.join(split_path, category)\n        if not os.path.isdir(category_dir):\n            continue\n        rgb_category_dir = os.path.join(rgb_split_path, category)\n        for file in tqdm(os.listdir(category_dir), desc=f\"Converting {split}/{category}\"):\n            if file.endswith(\".npy\"):\n                hsi_path = os.path.join(category_dir, file)\n                # Load the hyperspectral image (shape: (50, 128, 128))\n                hsi_img = np.load(hsi_path)\n                # Convert to RGB\n                rgb_img = hsi_to_rgb(hsi_img)\n                # Save as PNG file\n                rgb_save_path = os.path.join(rgb_category_dir, file.replace(\".npy\", \".png\"))\n                cv2.imwrite(rgb_save_path, cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR))  # OpenCV expects BGR order\n\nprint(\"✅ RGB conversion completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:50:58.486174Z","iopub.execute_input":"2025-04-15T13:50:58.486485Z","iopub.status.idle":"2025-04-15T13:51:00.812987Z","shell.execute_reply.started":"2025-04-15T13:50:58.486459Z","shell.execute_reply":"2025-04-15T13:51:00.812090Z"}},"outputs":[{"name":"stderr","text":"Converting train/Other: 100%|██████████| 160/160 [00:00<00:00, 251.30it/s]\nConverting train/Health: 100%|██████████| 160/160 [00:00<00:00, 263.20it/s]\nConverting train/Rust: 100%|██████████| 160/160 [00:00<00:00, 266.94it/s]\nConverting val/Other: 100%|██████████| 20/20 [00:00<00:00, 263.79it/s]\nConverting val/Health: 100%|██████████| 20/20 [00:00<00:00, 267.67it/s]\nConverting val/Rust: 100%|██████████| 20/20 [00:00<00:00, 274.55it/s]\nConverting test/Other: 100%|██████████| 20/20 [00:00<00:00, 262.62it/s]\nConverting test/Health: 100%|██████████| 20/20 [00:00<00:00, 280.48it/s]\nConverting test/Rust: 100%|██████████| 20/20 [00:00<00:00, 274.86it/s]","output_type":"stream"},{"name":"stdout","text":"✅ RGB conversion completed!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass DualModalityDataset(Dataset):\n    def __init__(self, hsi_root, rgb_root, transform_hsi=None, transform_rgb=None):\n        \"\"\"\n        Args:\n            hsi_root (str): Root directory of hyperspectral images (e.g., /kaggle/working/split_dataset/train)\n            rgb_root (str): Root directory of RGB images (e.g., /kaggle/working/rgb_split_dataset/train)\n            transform_hsi (callable, optional): Transformation to apply to the hyperspectral tensor.\n            transform_rgb (callable, optional): Transformation to apply to the RGB image (PIL/Image tensor).\n        \"\"\"\n        self.samples = []\n        self.transform_hsi = transform_hsi\n        self.transform_rgb = transform_rgb\n        \n        # Walk over each category folder in hsi_root\n        for category in sorted(os.listdir(hsi_root)):\n            hsi_category_path = os.path.join(hsi_root, category)\n            rgb_category_path = os.path.join(rgb_root, category)\n            if not os.path.isdir(hsi_category_path):\n                continue\n            # List all files in the HSI folder\n            for filename in os.listdir(hsi_category_path):\n                if filename.endswith(\".npy\"):\n                    hsi_path = os.path.join(hsi_category_path, filename)\n                    # Assume matching RGB file has same base filename with .png extension\n                    rgb_path = os.path.join(rgb_category_path, filename.replace(\".npy\", \".png\"))\n                    # Only add if the RGB image exists\n                    if os.path.exists(rgb_path):\n                        self.samples.append((hsi_path, rgb_path, category))\n        \n        # Create a mapping from category to an index\n        categories = sorted({s[2] for s in self.samples})\n        self.cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        hsi_path, rgb_path, category = self.samples[idx]\n        # Load hyperspectral image (stored as .npy, shape (50, 128, 128))\n        hsi_img = np.load(hsi_path)\n        # Convert to torch tensor (float32)\n        hsi_tensor = torch.tensor(hsi_img, dtype=torch.float32)\n        if self.transform_hsi:\n            hsi_tensor = self.transform_hsi(hsi_tensor)\n        \n        # Load RGB image using PIL (to allow use of torchvision transforms)\n        rgb_img = Image.open(rgb_path).convert(\"RGB\")\n        if self.transform_rgb:\n            rgb_img = self.transform_rgb(rgb_img)\n        else:\n            # By default, convert the image to tensor in [0, 1]\n            from torchvision.transforms import ToTensor\n            rgb_img = ToTensor()(rgb_img)\n        \n        label = self.cat2idx[category]\n        return hsi_tensor, rgb_img, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:54:17.020799Z","iopub.execute_input":"2025-04-15T13:54:17.021140Z","iopub.status.idle":"2025-04-15T13:54:17.029303Z","shell.execute_reply.started":"2025-04-15T13:54:17.021117Z","shell.execute_reply":"2025-04-15T13:54:17.028651Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import os\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as T\nimport torch\nimport torchvision.transforms.functional as TF\n\n# Custom resize transform for hyperspectral data (if tensor input)\n\n#transform_hsi = TensorResize((224, 224))  # Match both HSI and RGB resolutions\ntransform_rgb = T.Compose([\n    T.Resize((128, 128)),\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomVerticalFlip(p=0.5),  # Random vertical flip\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n    T.RandomRotation(30),  # Random rotation by up to 30 degrees\n    T.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random affine transformation\n    T.RandomCrop(128, padding=8),  # Random crop with padding\n    T.RandomPerspective(distortion_scale=0.2, p=0.5),  # Random perspective transformation\n    T.GaussianBlur(kernel_size=5),  # Apply Gaussian blur\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for pre-trained models\n])\n\n\n\n# Define root directories for each split\nhsi_train_root = os.path.join(split_dataset_path, \"train\")\nrgb_train_root = os.path.join(rgb_split_dataset_path, \"train\")\n\nhsi_val_root = os.path.join(split_dataset_path, \"val\")\nrgb_val_root = os.path.join(rgb_split_dataset_path, \"val\")\n\nhsi_test_root = os.path.join(split_dataset_path, \"test\")\nrgb_test_root = os.path.join(rgb_split_dataset_path, \"test\")\n\n# Instantiate datasets\ntrain_dataset = DualModalityDataset(hsi_root=hsi_train_root, rgb_root=rgb_train_root,\n                                    transform_hsi=transform_hsi, transform_rgb=transform_rgb)\n\nval_dataset = DualModalityDataset(hsi_root=hsi_val_root, rgb_root=rgb_val_root,\n                                  transform_hsi=transform_hsi, transform_rgb=transform_rgb)\n\ntest_dataset = DualModalityDataset(hsi_root=hsi_test_root, rgb_root=rgb_test_root,\n                                   transform_hsi=transform_hsi, transform_rgb=transform_rgb)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# Quick sanity check\nfor hsi_batch, rgb_batch, labels in val_loader:\n    print(\"Val HSI batch shape:\", hsi_batch.shape)\n    print(\"Val RGB batch shape:\", rgb_batch.shape)\n    print(\"Val Labels:\", labels)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:54:20.197218Z","iopub.execute_input":"2025-04-15T13:54:20.197501Z","iopub.status.idle":"2025-04-15T13:54:27.286052Z","shell.execute_reply.started":"2025-04-15T13:54:20.197479Z","shell.execute_reply":"2025-04-15T13:54:27.285155Z"}},"outputs":[{"name":"stdout","text":"Val HSI batch shape: torch.Size([32, 100, 224, 224])\nVal RGB batch shape: torch.Size([32, 3, 128, 128])\nVal Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1])\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### Doing SSL","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Paths\nsplit_dataset_path = \"/kaggle/working/split_dataset\"  # Where train/val/test exist\nssl_dataset_path = \"/kaggle/working/ssl_dataset\"  # New dataset for SSL\n\n# Create SSL dataset directory\nos.makedirs(ssl_dataset_path, exist_ok=True)\n\n# Merge all splits into SSL dataset\nfor split in [\"train\", \"val\", \"test\"]:\n    split_path = os.path.join(split_dataset_path, split)\n    if not os.path.exists(split_path):\n        continue  # Skip if path doesn't exist\n\n    for category in os.listdir(split_path):  # Loop over classes (Health, Rust, Other)\n        category_path = os.path.join(ssl_dataset_path, category)\n        os.makedirs(category_path, exist_ok=True)  # Ensure class folder exists\n\n        # Move all images to the SSL dataset\n        for file in os.listdir(os.path.join(split_path, category)):\n            src = os.path.join(split_path, category, file)\n            dest = os.path.join(category_path, file)\n            shutil.move(src, dest)\n\nprint(f\"✅ SSL dataset is ready in {ssl_dataset_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:09:03.215814Z","iopub.execute_input":"2025-04-15T13:09:03.216620Z","iopub.status.idle":"2025-04-15T13:09:03.241253Z","shell.execute_reply.started":"2025-04-15T13:09:03.216584Z","shell.execute_reply":"2025-04-15T13:09:03.240370Z"}},"outputs":[{"name":"stdout","text":"✅ SSL dataset is ready in /kaggle/working/ssl_dataset\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Data Augmentation for SSL","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport albumentations as A\nimport cv2\nimport os\nfrom glob import glob\n\n# Define augmentations\naugmentations = A.Compose([\n    A.RandomCrop(height=128, width=128, always_apply=True),  # Crop to 128x128\n    A.HorizontalFlip(p=0.5),  # Flip 50% of the time\n    A.VerticalFlip(p=0.5),  # Flip 50% of the time\n])\n\n# Function to apply augmentation\ndef apply_augmentations(img_path):\n    img = np.load(img_path)  # Load .npy file\n    img = np.float32(img)  # Ensure correct dtype for Albumentations\n    \n    # Normalize if needed (optional, depending on your dataset)\n    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)  # Scale to 0-255\n    img = img.astype(np.uint8)  # Convert to uint8 if necessary\n\n    # Albumentations expects HWC format, so ensure the correct shape\n    if img.ndim == 2:  \n        img = np.expand_dims(img, axis=-1)  # Convert grayscale to HWC\n    elif img.shape[0] < img.shape[-1]:  # If channels are first, swap axes\n        img = np.transpose(img, (1, 2, 0))\n\n    augmented = augmentations(image=img)[\"image\"]  # Apply augmentations\n    \n    return augmented\n\n# Apply augmentations to SSL dataset\nfor class_folder in os.listdir(ssl_dataset_path):\n    class_path = os.path.join(ssl_dataset_path, class_folder)\n    if not os.path.isdir(class_path):  # Skip non-folder items\n        continue\n\n    for img_path in glob(os.path.join(class_path, \"*.npy\")):\n        augmented_img = apply_augmentations(img_path)\n        np.save(img_path.replace(\".npy\", \"_aug.npy\"), augmented_img)  # Save augmented image\n\nprint(\"✅ Augmentations applied to SSL dataset!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:09:09.029228Z","iopub.execute_input":"2025-04-15T13:09:09.029542Z","iopub.status.idle":"2025-04-15T13:09:16.852928Z","shell.execute_reply.started":"2025-04-15T13:09:09.029516Z","shell.execute_reply":"2025-04-15T13:09:16.851961Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"},{"name":"stdout","text":"✅ Augmentations applied to SSL dataset!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### SSL Training with SimCLR","metadata":{}},{"cell_type":"markdown","source":"### AHAM Addition","metadata":{}},{"cell_type":"markdown","source":"NTXent loss - Loss function for SimCLR encoder","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom timm.models import create_model  \n\n# -------------------------\n# GPU/Memory Setup\n# -------------------------\ntorch.cuda.empty_cache()\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'\n\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------\n# Define AHAM Module\n# -------------------------\nclass AHAM(nn.Module):\n    \"\"\"\n    Adaptive Hyperspectral Attention Module (AHAM).\n    Expects input x of shape [B, P, D] where:\n      B = batch size,\n      P = number of tokens/patches,\n      D = embedding dimension.\n    Returns a weighted average over tokens (i.e. a refined feature vector)\n    and the attention weights (for visualization).\n    \"\"\"\n    def __init__(self, embed_dim, reduction=16):\n        super(AHAM, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Linear(embed_dim, embed_dim // reduction)\n        self.fc2 = nn.Linear(embed_dim // reduction, embed_dim)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: [B, P, D]\n        attention = self.avg_pool(x.transpose(1, 2)).squeeze(-1)  # [B, D]\n        attention = self.fc1(attention)\n        attention = nn.functional.relu(attention)\n        attention = self.fc2(attention)\n        attention = self.sigmoid(attention)  # [B, D]\n        x_weighted = (x * attention.unsqueeze(1)).mean(dim=1)  # [B, D]\n        return x_weighted, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:09:18.235847Z","iopub.execute_input":"2025-04-15T13:09:18.236142Z","iopub.status.idle":"2025-04-15T13:09:21.730496Z","shell.execute_reply.started":"2025-04-15T13:09:18.236118Z","shell.execute_reply":"2025-04-15T13:09:21.729353Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda:1\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# -------------------------\n# Define NT-Xent Loss (Contrastive Loss)\n# -------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass NTXentLossFull(nn.Module):\n    \"\"\"\n    A full NT-Xent loss where each sample is compared against all in the batch\n    (2B in total, minus itself).\n    This is the canonical approach in SimCLR.\n    \"\"\"\n    def __init__(self, temperature=0.5):\n        super().__init__()\n        self.temperature = temperature\n\n    def forward(self, z_i, z_j):\n        \"\"\"\n        z_i: [B, D]\n        z_j: [B, D]\n        Returns scalar contrastive loss.\n        \"\"\"\n        # If the inputs are 1D, unsqueeze them to add the batch dimension.\n        # if z_i.dim() == 1:\n        #     z_i = z_i.unsqueeze(0)\n        # if z_j.dim() == 1:\n        #     z_j = z_j.unsqueeze(0)\n        \n        # z_i = z_i.float()\n        # z_j = z_j.float()\n        z_i = F.normalize(z_i, dim=1)\n        z_j = F.normalize(z_j, dim=1)\n\n        # Concatenate along batch dimension\n        z = torch.cat([z_i, z_j], dim=0)  # [2B, D]\n\n        # Compute similarity matrix\n        sim_matrix = torch.matmul(z, z.T)  # [2B, 2B]\n\n        # Create labels for positives:\n        # For sample i in [0..B-1], the positive is i+B\n        # For sample i in [B..2B-1], the positive is i-B\n        batch_size = z_i.size(0)\n        labels = torch.arange(batch_size, device=z.device)\n        labels = torch.cat([labels + batch_size, labels], dim=0)  # [2B]\n\n        # Mask out self-similarity by setting the diagonal to large negative\n        mask = torch.eye(2 * batch_size, device=z.device).bool()\n        sim_matrix.masked_fill_(mask, -9e15)\n\n        # Scale by temperature\n        sim_matrix /= self.temperature\n\n        # Use cross-entropy over the row to find the positive sample.\n        loss = F.cross_entropy(sim_matrix, labels)\n        return loss\n\n\n\ncriterion = NTXentLossFull()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:09:38.627273Z","iopub.execute_input":"2025-04-15T13:09:38.627649Z","iopub.status.idle":"2025-04-15T13:09:38.634260Z","shell.execute_reply.started":"2025-04-15T13:09:38.627615Z","shell.execute_reply":"2025-04-15T13:09:38.633338Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# -------------------------\n# Define the SimCLR Model with AHAM Integrated\n# -------------------------\nclass SimCLR(nn.Module):\n    def __init__(self, backbone=\"vit_base_patch16_224\", feature_dim=128, input_channels=100):\n        \"\"\"\n        backbone: name of the backbone ViT model.\n        feature_dim: Final embedding dimension for contrastive learning.\n        input_channels: Number of channels in the hyperspectral input.\n        \"\"\"\n        super(SimCLR, self).__init__()\n        # Create the ViT model without classifier.\n        # Remove 'output_cls_token' because it is not supported.\n        self.encoder = create_model(backbone, pretrained=False, num_classes=0)\n        \n        # Modify patch embedding projection layer to accept input_channels\n        new_conv_layer = nn.Conv2d(\n            input_channels,\n            self.encoder.patch_embed.proj.out_channels,\n            kernel_size=self.encoder.patch_embed.proj.kernel_size,\n            stride=self.encoder.patch_embed.proj.stride,\n            padding=self.encoder.patch_embed.proj.padding,\n            bias=False\n        )\n        self.encoder.patch_embed.proj = new_conv_layer\n\n        # Integrate AHAM.\n        self.aham = AHAM(embed_dim=self.encoder.num_features)\n        # Project refined features to contrastive embedding space.\n        self.projector = nn.Sequential(\n            nn.Linear(self.encoder.num_features, 512),\n            nn.ReLU(),\n            nn.Linear(512, feature_dim)\n        )\n\n    def forward(self, x, return_attention=False):\n        # Get token embeddings from the encoder; x shape: [B, input_channels, 224, 224]\n        tokens = self.encoder.forward_features(x)  # Output shape: [B, P, D]\n        x_weighted, attention = self.aham(tokens)      # [B, D] and [B, D]\n        projections = self.projector(x_weighted)\n        if return_attention:\n            return projections, attention\n        return projections\n\nhsi_model = SimCLR(input_channels=100).to(device)\nhsi_model.train()\nprint(\"✅ SimCLR with AHAM is ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:09:43.118798Z","iopub.execute_input":"2025-04-15T13:09:43.119087Z","iopub.status.idle":"2025-04-15T13:09:45.035748Z","shell.execute_reply.started":"2025-04-15T13:09:43.119065Z","shell.execute_reply":"2025-04-15T13:09:45.034951Z"}},"outputs":[{"name":"stdout","text":"✅ SimCLR with AHAM is ready!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# -------------------------\n# Define Custom Transforms for the SSL Dataset\n# -------------------------\nclass RandomHorizontalFlipTensor:\n    \"\"\"Randomly horizontally flips a tensor image with probability p.\"\"\"\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, img):\n        if torch.rand(1).item() < self.p:\n            return torch.flip(img, dims=[-1])\n        return img\n\nclass RandomVerticalFlipTensor:\n    \"\"\"Randomly vertically flips a tensor image with probability p.\"\"\"\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, img):\n        if torch.rand(1).item() < self.p:\n            return torch.flip(img, dims=[-2])\n        return img\n\nclass RandomRotationTensor:\n    \"\"\"Randomly rotates a tensor image by an angle between -max_angle and max_angle degrees.\"\"\"\n    def __init__(self, max_angle=30):\n        self.max_angle = max_angle\n    def __call__(self, img):\n        angle = torch.empty(1).uniform_(-self.max_angle, self.max_angle).item()\n        return TF.rotate(img, angle)\n\nclass AddGaussianNoise:\n    \"\"\"Adds Gaussian noise to a tensor image.\"\"\"\n    def __init__(self, mean=0.0, std=0.1):\n        self.mean = mean\n        self.std = std\n    def __call__(self, img):\n        noise = torch.randn_like(img) * self.std + self.mean\n        return img + noise\n\nclass RandomContrastTensor:\n    \"\"\"Randomly adjusts contrast for a tensor image by a factor chosen uniformly.\"\"\"\n    def __init__(self, lower=0.7, upper=1.3):\n        self.lower = lower\n        self.upper = upper\n    def __call__(self, img):\n        factor = torch.empty(1).uniform_(self.lower, self.upper).item()\n        mean = torch.mean(img, dim=[1,2], keepdim=True)\n        return (img - mean) * factor + mean\n\nclass TensorResize:\n    \"\"\"Resizes a tensor image to the target size.\"\"\"\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img):\n        return torch.nn.functional.interpolate(\n            img.unsqueeze(0), size=self.size, mode='bilinear', align_corners=False\n        ).squeeze(0)\n\n# Compose an augmentation pipeline with many augmentations.\ntransform = T.Compose([\n    TensorResize((224, 224)),\n    RandomHorizontalFlipTensor(p=0.5),\n    RandomVerticalFlipTensor(p=0.5),\n    RandomRotationTensor(max_angle=30),\n    AddGaussianNoise(mean=0.0, std=0.05),\n    RandomContrastTensor(lower=0.8, upper=1.2)\n])\n\n# -------------------------\n# Define the SSL Dataset for Contrastive Learning\n# -------------------------\nclass SSLDataset(Dataset):\n    def __init__(self, root_dir, transform=None, target_channels=100):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.target_channels = target_channels\n        self.image_paths = [\n            os.path.join(root, f)\n            for root, _, files in os.walk(root_dir)\n            for f in files if f.endswith(\".npy\")\n        ]\n        print(f\"Found {len(self.image_paths)} hyperspectral images in {root_dir}\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = np.load(img_path)  # Load hyperspectral image (H, W, C)\n        if len(img.shape) == 2:\n            raise ValueError(f\"Image at {img_path} is grayscale (expected HxWxC).\")\n        C, H, W = img.shape[2], img.shape[0], img.shape[1]\n        if C < self.target_channels:\n            img_padded = np.zeros((H, W, self.target_channels), dtype=np.float32)\n            img_padded[:, :, :C] = img\n            img = img_padded\n        elif C > self.target_channels:\n            img = img[:, :, :self.target_channels]\n        img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n        img_min, img_max = img.min(), img.max()\n        if img_max > img_min:\n            img = (img - img_min) / (img_max - img_min)\n        else:\n            img = np.zeros_like(img)\n        img_tensor = torch.tensor(img, dtype=torch.float32)\n        # Generate two different augmented views.\n        if self.transform:\n            view1 = self.transform(img_tensor)\n            view2 = self.transform(img_tensor)\n        else:\n            view1, view2 = img_tensor, img_tensor\n        return view1, view2\n\n# -------------------------\n# Instantiate the Dataset and DataLoader\n# -------------------------\n# Update this path to point to your hyperspectral .npy files.\n#ssl_dataset_path = \"/path/to/hsi_dataset\"\nssl_dataset = SSLDataset(ssl_dataset_path, transform)\nssl_loader = DataLoader(ssl_dataset, batch_size=32, shuffle=True, num_workers=2)\n\n# -------------------------\n# Define the Optimizer\n# -------------------------\noptimizer = optim.Adam(hsi_model.parameters(), lr=3e-4)\n\n# -------------------------\n# Training Loop for Self-Supervised Learning (with AHAM integrated)\n# -------------------------\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    hsi_model.train()\n    epoch_loss = 0.0\n    for images1, images2 in ssl_loader:\n        images1, images2 = images1.to(device), images2.to(device)\n        z_i, z_j = hsi_model(images1), hsi_model(images2)\n        loss = criterion(z_i, z_j)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * images1.size(0)\n    avg_loss = epoch_loss / len(ssl_dataset)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\nprint(\"✅ Self-Supervised Learning (SSL) Training Complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:10:07.792469Z","iopub.execute_input":"2025-04-15T13:10:07.792795Z","iopub.status.idle":"2025-04-15T13:47:20.225408Z","shell.execute_reply.started":"2025-04-15T13:10:07.792769Z","shell.execute_reply":"2025-04-15T13:47:20.224096Z"}},"outputs":[{"name":"stdout","text":"Found 1200 hyperspectral images in /kaggle/working/ssl_dataset\nEpoch [1/10], Loss: 3.3252\nEpoch [2/10], Loss: 2.8290\nEpoch [3/10], Loss: 2.6887\nEpoch [4/10], Loss: 2.5996\nEpoch [5/10], Loss: 2.5742\nEpoch [6/10], Loss: 2.5279\nEpoch [7/10], Loss: 2.4973\nEpoch [8/10], Loss: 2.4992\nEpoch [9/10], Loss: 2.4749\nEpoch [10/10], Loss: 2.4741\n✅ Self-Supervised Learning (SSL) Training Complete!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Fine Tuning for Crop Health Classification","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass DualStreamFusionModel(nn.Module):\n    def __init__(self, hsi_ssl_model, num_classes=3):\n        \"\"\"\n        hsi_ssl_model: your already-trained SimCLR-style model for hyperspectral data.\n        num_classes: number of crop health classes.\n        \"\"\"\n        super(DualStreamFusionModel, self).__init__()\n\n        # HSI Branch: use your trained SSL model.\n        # Freeze its weights so that only the new fusion head is trained.\n        self.hsi_encoder = hsi_ssl_model  \n        for param in self.hsi_encoder.parameters():\n            param.requires_grad = False\n        \n        # RGB branch: use a pretrained ResNet18.\n        self.rgb_encoder = models.resnet18(pretrained=True)\n        # Remove the default classifier so that we only obtain features.\n        self.rgb_encoder.fc = nn.Identity()  # Outputs a 512-D feature vector\n        \n        # Fusion Head: Combine the 128-D (HSI) and 512-D (RGB) features.\n        self.classifier = nn.Sequential(\n            nn.Linear(128 + 512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, hsi, rgb):\n        # print(\"FusionModel.forward() called with:\")\n        # print(\" - hsi type:\", type(hsi), \"shape:\", hsi.shape)\n        # print(\" - rgb type:\", type(rgb), \"shape:\", rgb.shape)\n        hsi_features = self.hsi_encoder(hsi)\n        rgb_features = self.rgb_encoder(rgb)\n        #transform_hsi = TensorResize((224, 224))\n        fused_features = torch.cat([hsi_features, rgb_features], dim=1)\n        logits = self.classifier(fused_features)\n        return logits\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:54:39.654687Z","iopub.execute_input":"2025-04-15T13:54:39.655009Z","iopub.status.idle":"2025-04-15T13:54:39.661796Z","shell.execute_reply.started":"2025-04-15T13:54:39.654986Z","shell.execute_reply":"2025-04-15T13:54:39.660921Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Assume hsi_ssl_model is your trained SSL model with SimCLR architecture.\n# If using the same 'device' as in training:\n#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nhsi_model.to(device)\nhsi_model.eval()  # Set to evaluation mode\n\n# Create the fusion model.\nnum_classes = 3  # Adjust based on your dataset.\nfusion_model = DualStreamFusionModel(hsi_model, num_classes=num_classes).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:49:24.138229Z","iopub.execute_input":"2025-04-15T13:49:24.138508Z","iopub.status.idle":"2025-04-15T13:49:24.797404Z","shell.execute_reply.started":"2025-04-15T13:49:24.138487Z","shell.execute_reply":"2025-04-15T13:49:24.796521Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 151MB/s] \n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport shutil\n\ndef delete_all_in_working(dir_path=\"/kaggle/working\"):\n    # List all items in the directory.\n    for filename in os.listdir(dir_path):\n        file_path = os.path.join(dir_path, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)  # Remove file or link\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)  # Remove directory and its contents\n            print(f\"Deleted: {file_path}\")\n        except Exception as e:\n            print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n# Call the function (will delete everything under /kaggle/working)\ndelete_all_in_working()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:49:29.246241Z","iopub.execute_input":"2025-04-15T13:49:29.246576Z","iopub.status.idle":"2025-04-15T13:49:29.251802Z","shell.execute_reply.started":"2025-04-15T13:49:29.246528Z","shell.execute_reply":"2025-04-15T13:49:29.251101Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(fusion_model.parameters(), lr=1e-4)  # Only parameters in classifier & rgb_encoder will update.\n\n# num_epochs = 10\n\n# for epoch in range(num_epochs):\n#     fusion_model.train()\n#     running_loss = 0.0\n#     total_samples = 0\n#     correct_predictions = 0\n    \n#     for hsi_imgs, rgb_imgs, labels in train_loader:\n#         hsi_imgs = hsi_imgs.to(device)   # (B, 50, H, W)\n#         rgb_imgs = rgb_imgs.to(device)   # (B, 3, H, W)\n#         labels = labels.to(device)\n        \n#         optimizer.zero_grad()\n#         outputs = fusion_model(hsi_imgs, rgb_imgs)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n        \n#         running_loss += loss.item() * hsi_imgs.size(0)\n#         total_samples += hsi_imgs.size(0)\n#         _, preds = torch.max(outputs, 1)\n#         correct_predictions += (preds == labels).sum().item()\n    \n#     epoch_loss = running_loss / total_samples\n#     epoch_acc = correct_predictions / total_samples\n#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:54:47.992159Z","iopub.execute_input":"2025-04-15T13:54:47.992473Z","iopub.status.idle":"2025-04-15T13:54:48.001198Z","shell.execute_reply.started":"2025-04-15T13:54:47.992447Z","shell.execute_reply":"2025-04-15T13:54:48.000334Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef evaluate_model(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for hsi_imgs, rgb_imgs, labels in dataloader:\n            hsi_imgs = hsi_imgs.to(device)\n            rgb_imgs = rgb_imgs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(hsi_imgs, rgb_imgs)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * hsi_imgs.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct_predictions += (preds == labels).sum().item()\n            total_samples += hsi_imgs.size(0)\n\n    avg_loss = running_loss / total_samples\n    accuracy = correct_predictions / total_samples\n    return avg_loss, accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:54:51.112153Z","iopub.execute_input":"2025-04-15T13:54:51.112483Z","iopub.status.idle":"2025-04-15T13:54:51.118168Z","shell.execute_reply.started":"2025-04-15T13:54:51.112453Z","shell.execute_reply":"2025-04-15T13:54:51.117281Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"val_losses, val_accuracies = [], []\noptimizer = optim.Adam(fusion_model.parameters(), lr=1e-4, weight_decay=1e-5)\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    fusion_model.train()\n    running_loss = 0.0\n    total_samples = 0\n    correct_predictions = 0\n\n    for hsi_imgs, rgb_imgs, labels in train_loader:\n        hsi_imgs = hsi_imgs.to(device)\n        rgb_imgs = rgb_imgs.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = fusion_model(hsi_imgs, rgb_imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * hsi_imgs.size(0)\n        total_samples += hsi_imgs.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct_predictions += (preds == labels).sum().item()\n\n    epoch_loss = running_loss / total_samples\n    epoch_acc = correct_predictions / total_samples\n\n    val_loss, val_acc = evaluate_model(fusion_model, val_loader, criterion, device)\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, \"\n          f\"Train Acc: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:54:58.849017Z","iopub.execute_input":"2025-04-15T13:54:58.849324Z","iopub.status.idle":"2025-04-15T14:13:56.897880Z","shell.execute_reply.started":"2025-04-15T13:54:58.849299Z","shell.execute_reply":"2025-04-15T14:13:56.896801Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/20], Train Loss: 1.0513, Train Acc: 0.4313, Val Loss: 0.9740, Val Acc: 0.5667\nEpoch [2/20], Train Loss: 0.9278, Train Acc: 0.5354, Val Loss: 0.8940, Val Acc: 0.5333\nEpoch [3/20], Train Loss: 0.8678, Train Acc: 0.5750, Val Loss: 0.8523, Val Acc: 0.6167\nEpoch [4/20], Train Loss: 0.8560, Train Acc: 0.5813, Val Loss: 0.8117, Val Acc: 0.6167\nEpoch [5/20], Train Loss: 0.8149, Train Acc: 0.5917, Val Loss: 0.8787, Val Acc: 0.6000\nEpoch [6/20], Train Loss: 0.8114, Train Acc: 0.6062, Val Loss: 0.9303, Val Acc: 0.6167\nEpoch [7/20], Train Loss: 0.7618, Train Acc: 0.6396, Val Loss: 0.8460, Val Acc: 0.6333\nEpoch [8/20], Train Loss: 0.7731, Train Acc: 0.6188, Val Loss: 0.7272, Val Acc: 0.6500\nEpoch [9/20], Train Loss: 0.7204, Train Acc: 0.6667, Val Loss: 0.8844, Val Acc: 0.6000\nEpoch [10/20], Train Loss: 0.7331, Train Acc: 0.6521, Val Loss: 0.8716, Val Acc: 0.6333\nEpoch [11/20], Train Loss: 0.7048, Train Acc: 0.6521, Val Loss: 0.9048, Val Acc: 0.6000\nEpoch [12/20], Train Loss: 0.6826, Train Acc: 0.6875, Val Loss: 0.8691, Val Acc: 0.6333\nEpoch [13/20], Train Loss: 0.6437, Train Acc: 0.7063, Val Loss: 0.9206, Val Acc: 0.6000\nEpoch [16/20], Train Loss: 0.5950, Train Acc: 0.7396, Val Loss: 0.9608, Val Acc: 0.5667\nEpoch [17/20], Train Loss: 0.6132, Train Acc: 0.7146, Val Loss: 0.9366, Val Acc: 0.6000\nEpoch [18/20], Train Loss: 0.5822, Train Acc: 0.7292, Val Loss: 0.9507, Val Acc: 0.6000\nEpoch [19/20], Train Loss: 0.6096, Train Acc: 0.7479, Val Loss: 0.9665, Val Acc: 0.6333\nEpoch [20/20], Train Loss: 0.5454, Train Acc: 0.7583, Val Loss: 0.9659, Val Acc: 0.6333\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef evaluate_on_test(model, test_loader, criterion, device, class_names=None):\n    model.eval()\n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for hsi_imgs, rgb_imgs, labels in test_loader:\n            hsi_imgs = hsi_imgs.to(device)\n            rgb_imgs = rgb_imgs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(hsi_imgs, rgb_imgs)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * hsi_imgs.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct_predictions += (preds == labels).sum().item()\n            total_samples += hsi_imgs.size(0)\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n\n    avg_loss = running_loss / total_samples\n    accuracy = correct_predictions / total_samples\n\n    print(f\"\\nTest Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n\n    # Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=class_names if class_names else 'auto',\n                yticklabels=class_names if class_names else 'auto')\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.show()\n\n    # Classification Report\n    print(\"\\nClassification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=class_names if class_names else None))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:13:56.899439Z","iopub.execute_input":"2025-04-15T14:13:56.899742Z","iopub.status.idle":"2025-04-15T14:13:57.204096Z","shell.execute_reply.started":"2025-04-15T14:13:56.899715Z","shell.execute_reply":"2025-04-15T14:13:57.203447Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class_names = [\"Health\",\"Other\",\"Rust\"]\nevaluate_on_test(fusion_model, test_loader, criterion, device, class_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:13:57.205424Z","iopub.execute_input":"2025-04-15T14:13:57.205907Z","iopub.status.idle":"2025-04-15T14:14:05.786060Z","shell.execute_reply.started":"2025-04-15T14:13:57.205873Z","shell.execute_reply":"2025-04-15T14:14:05.785216Z"}},"outputs":[{"name":"stdout","text":"\nTest Loss: 0.9595, Test Accuracy: 0.6167\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAoAAAAIjCAYAAACTRapjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLRUlEQVR4nO3dfXzO9f////uxsWMzO3M6w06c5PwkZyVl9mk5CZEihYbOhMRKrHKyiZW3k6EyUUj09q6QTt4iZ0uk0FDkdOiEyNne2zjM9vr90c/x7Whomx07Dnvdrl1el4vj+Xodz+fjtffxnofH8/l6HhbDMAwBAADANDxcHQAAAACKFwkgAACAyZAAAgAAmAwJIAAAgMmQAAIAAJgMCSAAAIDJkAACAACYDAkgAACAyZAAAgAAmAwJIIDrOnDggNq3b6+AgABZLBatWLGiSPs/cuSILBaLFixYUKT93szatWundu3auToMACUYCSBwEzh06JCeeuop1ahRQ97e3vL391ebNm00Y8YMXbhwwaljx8TEaPfu3Zo4caIWLVqkFi1aOHW84tS/f39ZLBb5+/tf9ed44MABWSwWWSwWTZkypcD9//bbbxo/frxSU1OLIFoAKDqlXB0AgOv77LPP1LNnT1mtVj366KNq2LChLl26pE2bNmnkyJH68ccf9dZbbzll7AsXLmjLli166aWXNHToUKeMERYWpgsXLqh06dJO6f+flCpVSllZWfrkk0/Uq1cvh3OLFy+Wt7e3Ll68WKi+f/vtN8XHxys8PFxNmzbN9/tWr15dqPEAIL9IAAE3lpaWpt69eyssLEzr1q1TlSpV7OeGDBmigwcP6rPPPnPa+KdOnZIkBQYGOm0Mi8Uib29vp/X/T6xWq9q0aaP3338/TwK4ZMkSde7cWR999FGxxJKVlaUyZcrIy8urWMYDYF5MAQNubPLkycrIyNDbb7/tkPxdUatWLT377LP215cvX9aECRNUs2ZNWa1WhYeH68UXX5TNZnN4X3h4uLp06aJNmzapVatW8vb2Vo0aNfTuu+/arxk/frzCwsIkSSNHjpTFYlF4eLikP6dOr/z5r8aPHy+LxeLQtmbNGt15550KDAxU2bJlVadOHb344ov289daA7hu3Trddddd8vX1VWBgoLp166a9e/dedbyDBw+qf//+CgwMVEBAgAYMGKCsrKxr/2D/5pFHHtF///tfnTt3zt723Xff6cCBA3rkkUfyXH/mzBk9//zzatSokcqWLSt/f3916tRJO3futF+zYcMGtWzZUpI0YMAA+1Tylfts166dGjZsqO3bt6tt27YqU6aM/efy9zWAMTEx8vb2znP/HTp0UFBQkH777bd83ysASCSAgFv75JNPVKNGDd1xxx35uv7xxx/X2LFj1axZM02fPl2RkZFKTExU796981x78OBBPfjgg7rnnns0depUBQUFqX///vrxxx8lST169ND06dMlSQ8//LAWLVqkpKSkAsX/448/qkuXLrLZbEpISNDUqVN133336euvv77u+7788kt16NBBJ0+e1Pjx4xUbG6vNmzerTZs2OnLkSJ7re/Xqpf/9739KTExUr169tGDBAsXHx+c7zh49eshisWjZsmX2tiVLlqhu3bpq1qxZnusPHz6sFStWqEuXLpo2bZpGjhyp3bt3KzIy0p6M1atXTwkJCZKkJ598UosWLdKiRYvUtm1bez+nT59Wp06d1LRpUyUlJSkqKuqq8c2YMUMVK1ZUTEyMcnJyJElz5szR6tWrNWvWLIWEhOT7XgFAkmQAcEvnz583JBndunXL1/WpqamGJOPxxx93aH/++ecNSca6devsbWFhYYYkIyUlxd528uRJw2q1Gs8995y9LS0tzZBk/Otf/3LoMyYmxggLC8sTw7hx44y//lqZPn26Ick4derUNeO+Msb8+fPtbU2bNjUqVapknD592t62c+dOw8PDw3j00UfzjDdw4ECHPu+//36jfPny1xzzr/fh6+trGIZhPPjgg8bdd99tGIZh5OTkGMHBwUZ8fPxVfwYXL140cnJy8tyH1Wo1EhIS7G3fffddnnu7IjIy0pBkJCcnX/VcZGSkQ9sXX3xhSDJeeeUV4/Dhw0bZsmWN7t27/+M9AsDVUAEE3FR6erokyc/PL1/Xf/7555Kk2NhYh/bnnntOkvKsFaxfv77uuusu++uKFSuqTp06Onz4cKFj/rsrawc//vhj5ebm5us9x48fV2pqqvr3769y5crZ2xs3bqx77rnHfp9/NWjQIIfXd911l06fPm3/GebHI488og0bNujEiRNat26dTpw4cdXpX+nPdYMeHn/++szJydHp06ft09s7duzI95hWq1UDBgzI17Xt27fXU089pYSEBPXo0UPe3t6aM2dOvscCgL8iAQTclL+/vyTpf//7X76uP3r0qDw8PFSrVi2H9uDgYAUGBuro0aMO7aGhoXn6CAoK0tmzZwsZcV4PPfSQ2rRpo8cff1yVK1dW79699Z///Oe6yeCVOOvUqZPnXL169fTHH38oMzPTof3v9xIUFCRJBbqXe++9V35+flq6dKkWL16sli1b5vlZXpGbm6vp06erdu3aslqtqlChgipWrKhdu3bp/Pnz+R6zatWqBXrgY8qUKSpXrpxSU1M1c+ZMVapUKd/vBYC/IgEE3JS/v79CQkL0ww8/FOh9f38I41o8PT2v2m4YRqHHuLI+7QofHx+lpKToyy+/VL9+/bRr1y499NBDuueee/JceyNu5F6usFqt6tGjhxYuXKjly5dfs/onSZMmTVJsbKzatm2r9957T1988YXWrFmjBg0a5LvSKf358ymI77//XidPnpQk7d69u0DvBYC/IgEE3FiXLl106NAhbdmy5R+vDQsLU25urg4cOODQ/vvvv+vcuXP2J3qLQlBQkMMTs1f8vcooSR4eHrr77rs1bdo07dmzRxMnTtS6deu0fv36q/Z9Jc59+/blOffTTz+pQoUK8vX1vbEbuIZHHnlE33//vf73v/9d9cGZKz788ENFRUXp7bffVu/evdW+fXtFR0fn+ZnkNxnPj8zMTA0YMED169fXk08+qcmTJ+u7774rsv4BmAsJIODGXnjhBfn6+urxxx/X77//nuf8oUOHNGPGDEl/TmFKyvOk7rRp0yRJnTt3LrK4atasqfPnz2vXrl32tuPHj2v58uUO1505cybPe69siPz3rWmuqFKlipo2baqFCxc6JFQ//PCDVq9ebb9PZ4iKitKECRP0+uuvKzg4+JrXeXp65qkufvDBB/r1118d2q4kqldLlgtq1KhROnbsmBYuXKhp06YpPDxcMTEx1/w5AsD1sBE04MZq1qypJUuW6KGHHlK9evUcvglk8+bN+uCDD9S/f39JUpMmTRQTE6O33npL586dU2RkpL799lstXLhQ3bt3v+YWI4XRu3dvjRo1Svfff7+GDRumrKwszZ49W7fccovDQxAJCQlKSUlR586dFRYWppMnT+rNN99UtWrVdOedd16z/3/961/q1KmTWrdurccee0wXLlzQrFmzFBAQoPHjxxfZffydh4eHXn755X+8rkuXLkpISNCAAQN0xx13aPfu3Vq8eLFq1KjhcF3NmjUVGBio5ORk+fn5ydfXV7fddpsiIiIKFNe6dev05ptvaty4cfZtaebPn6927dppzJgxmjx5coH6AwC2gQFuAvv37zeeeOIJIzw83PDy8jL8/PyMNm3aGLNmzTIuXrxovy47O9uIj483IiIijNKlSxvVq1c34uLiHK4xjD+3gencuXOecf6+/ci1toExDMNYvXq10bBhQ8PLy8uoU6eO8d577+XZBmbt2rVGt27djJCQEMPLy8sICQkxHn74YWP//v15xvj7Vilffvml0aZNG8PHx8fw9/c3unbtauzZs8fhmivj/X2bmfnz5xuSjLS0tGv+TA3DcRuYa7nWNjDPPfecUaVKFcPHx8do06aNsWXLlqtu3/Lxxx8b9evXN0qVKuVwn5GRkUaDBg2uOuZf+0lPTzfCwsKMZs2aGdnZ2Q7XjRgxwvDw8DC2bNly3XsAgL+zGEYBVkkDAADgpscaQAAAAJMhAQQAADAZEkAAAACTIQEEAABwIykpKeratatCQkJksVi0YsUKh/MZGRkaOnSoqlWrJh8fH9WvX1/JyckFGoMEEAAAwI1kZmaqSZMmeuONN656PjY2VqtWrdJ7772nvXv3avjw4Ro6dKhWrlyZ7zF4ChgAAMBNWSwWLV++XN27d7e3NWzYUA899JDGjBljb2vevLk6deqkV155JV/9UgEEAABwIpvNpvT0dIfjRr7F54477tDKlSv166+/yjAMrV+/Xvv371f79u3z3UeJ/CaQD3ced3UIAOD29p7KdHUIgIMx0bVcNrbPrUOd1veobhUUHx/v0DZu3LhCf7PRrFmz9OSTT6patWoqVaqUPDw8NHfuXLVt2zbffZTIBBAAAMBdxMXFKTY21qHNarUWur9Zs2bpm2++0cqVKxUWFqaUlBQNGTJEISEhio6OzlcfJIAAAAAW562Ks1qtN5Tw/dWFCxf04osvavny5ercubMkqXHjxkpNTdWUKVNIAAEAAPLNYnF1BPmSnZ2t7OxseXg4Jqyenp7Kzc3Ndz8kgAAAAG4kIyNDBw8etL9OS0tTamqqypUrp9DQUEVGRmrkyJHy8fFRWFiYNm7cqHfffVfTpk3L9xgkgAAAAE6cAi6obdu2KSoqyv76yvrBmJgYLViwQP/+978VFxenPn366MyZMwoLC9PEiRM1aNCgfI9BAggAAOBG2rVrp+tt0xwcHKz58+ff0BgkgAAAADfJGsCi4j71TgAAABQLKoAAAAButAawOJjrbgEAAEAFEAAAwGxrAEkAAQAAmAIGAABASUYFEAAAwGRTwFQAAQAATIYKIAAAAGsAAQAAUJJRAQQAAGANIAAAAEoyKoAAAAAmWwNIAggAAMAUMAAAAEoyKoAAAAAmmwI2190CAACACiAAAAAVQAAAAJRoVAABAAA8eAoYAAAAJRgVQAAAAJOtASQBBAAAYCNoAAAAlGRUAAEAAEw2BWyuuwUAAAAVQAAAANYAAgAAoESjAggAAMAaQAAAAJRkVAABAABMtgaQBBAAAIApYAAAAJRkVAABAABMNgVMBRAAAMBkqAACAACwBhAAAAAlGRVAAAAA1gACAACgJCMBBAAAsHg47yiglJQUde3aVSEhIbJYLFqxYkWea/bu3av77rtPAQEB8vX1VcuWLXXs2LF8j0ECCAAA4EYJYGZmppo0aaI33njjqucPHTqkO++8U3Xr1tWGDRu0a9cujRkzRt7e3vkegzWAAAAAbqRTp07q1KnTNc+/9NJLuvfeezV58mR7W82aNQs0BhVAAAAAi8Vph81mU3p6usNhs9kKFWZubq4+++wz3XLLLerQoYMqVaqk22677arTxNdDAggAAOBEiYmJCggIcDgSExML1dfJkyeVkZGhV199VR07dtTq1at1//33q0ePHtq4cWO++2EKGAAAwIkbQcfFxSk2NtahzWq1Fqqv3NxcSVK3bt00YsQISVLTpk21efNmJScnKzIyMl/9kAACAAA4kdVqLXTC93cVKlRQqVKlVL9+fYf2evXqadOmTfnuhwQQAADgJtkI2svLSy1bttS+ffsc2vfv36+wsLB890MCCAAA4EYyMjJ08OBB++u0tDSlpqaqXLlyCg0N1ciRI/XQQw+pbdu2ioqK0qpVq/TJJ59ow4YN+R6DBBAAAMCJawALatu2bYqKirK/vrJ+MCYmRgsWLND999+v5ORkJSYmatiwYapTp44++ugj3XnnnfkegwQQAADAjaaA27VrJ8MwrnvNwIEDNXDgwEKP4TYJ4IEDB7R+/XqdPHnS/oTLFWPHjnVRVAAAACWPWySAc+fO1dNPP60KFSooODhYlr9k4RaLhQQQAAA4lcWNKoDFwS0SwFdeeUUTJ07UqFGjXB0KAABAiecWCeDZs2fVs2dPV4cBAABMymwVQLd45KVnz55avXq1q8MAAAAwBZdVAGfOnGn/c61atTRmzBh98803atSokUqXLu1w7bBhw4o7PAAAYCbmKgDKYvzTc8ZOEhERka/rLBaLDh8+XKC+P9x5vDAhAYCp7D2V6eoQAAdjomu5bGzfnvOd1nfmBwOc1ndhuawCmJaW5qqhAQAAHLAG0AUSEhKUlZWVp/3ChQtKSEhwQUQAAMBMLBaL0w535BYJYHx8vDIyMvK0Z2VlKT4+3gURAQAAlFxusQ2MYRhXzZB37typcuXKuSAiAABgJu5aqXMWlyaAQUFB9vLoLbfc4vDDz8nJUUZGhgYNGuTCCAEAAEoelyaASUlJMgxDAwcOVHx8vAICAuznvLy8FB4ertatW7swQgAAYAZUAItRTEyMpD+3hLnjjjvy7P8H1zl/5pS+eG+O9qd+q2zbRZUPrqoeg0epWs26rg4NJsVnEu4kNzdHuz5borTv1uti+ln5BJRTjduj1ahjb9MlErg5uSwBTE9Pt//51ltv1YULF3ThwoWrXuvv719cYUHShYz/6a0xQ1Wjwa2KefE1+foH6vTxX+Tj6+fq0GBSfCbhbvas/lAHvvpcrR8docAqYTp99IC2vJckL29f1Y26z9XhoTBMlre7LAEMDAz8x38lXXk4JCcnp5iigiSlfLxEAeUr6YHBo+1t5SpVcWFEMDs+k3A3p9L2qlrj21StYStJUtnylXVk+0b9cXSfiyMD8sdlCeD69etdNTT+wd5tm1W7SUu9P22c0vbslH+5CrqtfXe1jO7i6tBgUnwm4W4qRtTTga9XKf33X+VfuarO/nJYpw7tUfMej7s6NBSS2abuXZYARkZGumpo/IOzJ3/Tt2s+VpvOvRR5f1/9cugnfTp/pjxLlVKzdh1dHR5MiM8k3E2D9j2VfTFLKyc8JYvFQ4aRq6ZdH1VEqyhXhwbki1vsA3hFVlaWjh07pkuXLjm0N27c+JrvsdlsstlsDm3Zl2wq7WV1SoxmYOQaqlqzjto/8oQkKSSitk4eS9O3a1byly1cgs8k3M3RHV8p7bsNurP/SAVUCdPZXw5r20dvySegnGreHu3q8FAIZqsAusU3gZw6dUpdunSRn5+fGjRooFtvvdXhuJ7ExEQFBAQ4HMvfnlVMkZdMfkHlVbFamENbxWphOvfHSRdFBLPjMwl3s2P5O2rQvqfCW0QqqGq4atz2f6oX1V0/rv7A1aGhkPgqOBcYPny4zp07p61bt8rHx0erVq3SwoULVbt2ba1cufK6742Li9P58+cdjvsfe6aYIi+ZQus01B+//ezQ9sdvPyuoYmUXRQSz4zMJd3M525bnL3aLx59TwcDNwC0SwHXr1mnatGlq0aKFPDw8FBYWpr59+2ry5MlKTEy87nutVqv8/f0dDqZ/b0ybzj3184E92rDsPZ0+8Yt2bvpS3639VLd16O7q0GBSfCbhbqo1bKUfvliqX374Vhmnf9ex1M3au265qjfhywtuVmarAFoMwzBcHYS/v7927dql8PBwhYWFacmSJWrTpo3S0tLUoEEDZWVlFai/D3ced1Kk5vHT9s1avWSuTp/4RUGVqqhN5148cQmX4jNZ9PaeynR1CDet7ItZ2vnpe/o5dbMuZpyXT0A5hbeIVKNOD8uzFF9qUFhjomu5bOzyj77vtL5Pv/uw0/ouLLd4CKROnTrat2+fwsPD1aRJE82ZM0fh4eFKTk5WlSrs9eUKdZvfobrN73B1GIAdn0m4k9LeZdTiwSfV4sEnXR0Kiop7Fuqcxi0SwGeffVbHj/9ZtRs3bpw6duyoxYsXy8vLSwsWLHBtcAAAACWMWySAffv2tf+5efPmOnr0qH766SeFhoaqQoUKLowMAACYgbuu1XMWt3gI5IpLly5p37598vLyUrNmzUj+AAAAnMAtEsCsrCw99thjKlOmjBo0aKBjx45Jkp555hm9+uqrLo4OAACUdGZ7CtgtEsC4uDjt3LlTGzZskLe3t709OjpaS5cudWFkAADADMyWALrFGsAVK1Zo6dKluv322x1+UA0aNNChQ4dcGBkAAEDJ4xYJ4KlTp1SpUqU87ZmZmW6bOQMAgBLEZOmGW0wBt2jRQp999pn99ZWkb968eWrdml3VAQAAipJbVAAnTZqkTp06ac+ePbp8+bJmzJihPXv2aPPmzdq4caOrwwMAACWc2WYc3aICeOeddyo1NVWXL19Wo0aNtHr1alWqVElbtmxR8+bNXR0eAABAieLSCmB6err9zxUrVtTUqVOveo2/v39xhgUAAEzGbBVAlyaAgYGB1/2BG4Yhi8WinJycYowKAACgZHNpArh+/Xr7nw3D0L333qt58+apatWqLowKAACYDRXAYhQZGenw2tPTU7fffrtq1KjhoogAAIAZmS0BdIuHQAAAAFB83GIbGAAAAJcyVwHQ/SqAZivBAgAA/FVKSoq6du2qkJAQWSwWrVix4prXDho0SBaLRUlJSQUaw6UVwB49eji8vnjxogYNGiRfX1+H9mXLlhVnWAAAwGTcqQCVmZmpJk2aaODAgXlypb9avny5vvnmG4WEhBR4DJcmgAEBAQ6v+/bt66JIAAAA3EOnTp3UqVOn617z66+/6plnntEXX3yhzp07F3gMlyaA8+fPd+XwAAAAkpxbAbTZbLLZbA5tVqtVVqu1UP3l5uaqX79+GjlypBo0aFCoPtxuDSAAAEBJkpiYqICAAIcjMTGx0P299tprKlWqlIYNG1boPngKGAAAmJ4zK4BxcXGKjY11aCts9W/79u2aMWOGduzYcUMxUwEEAACwOO+wWq3y9/d3OAqbAH711Vc6efKkQkNDVapUKZUqVUpHjx7Vc889p/Dw8Hz3QwUQAADgJtGvXz9FR0c7tHXo0EH9+vXTgAED8t0PCSAAADA9d9oGJiMjQwcPHrS/TktLU2pqqsqVK6fQ0FCVL1/e4frSpUsrODhYderUyfcYJIAAAABuZNu2bYqKirK/vrJ+MCYmRgsWLCiSMUgAAQCA6blTBbBdu3YyDCPf1x85cqTAY/AQCAAAgMlQAQQAAKbnThXA4kAFEAAAwGSoAAIAANMzWwWQBBAAAMBc+R9TwAAAAGZDBRAAAJie2aaAqQACAACYDBVAAABgelQAAQAAUKJRAQQAAKZnsgIgFUAAAACzoQIIAABMz2xrAEkAAQCA6Zks/2MKGAAAwGyoAAIAANMz2xQwFUAAAACToQIIAABMz2QFQCqAAAAAZkMFEAAAmJ6Hh7lKgFQAAQAATIYKIAAAMD2zrQEkAQQAAKbHNjAAAAAo0agAAgAA0zNZAZAKIAAAgNlQAQQAAKbHGkAAAACUaFQAAQCA6VEBBAAAQIlGBRAAAJieyQqAJIAAAABMAQMAAKBEowIIAABMz2QFQCqAAAAAZkMFEAAAmB5rAAEAAFCiUQEEAACmZ7ICIBVAAAAAs6ECCAAATI81gAAAACjRSAABAIDpWSzOOwoqJSVFXbt2VUhIiCwWi1asWGE/l52drVGjRqlRo0by9fVVSEiIHn30Uf32228FGoMEEAAAmJ7FYnHaUVCZmZlq0qSJ3njjjTznsrKytGPHDo0ZM0Y7duzQsmXLtG/fPt13330FGoM1gAAAAG6kU6dO6tSp01XPBQQEaM2aNQ5tr7/+ulq1aqVjx44pNDQ0X2OQAAIAANNz5jMgNptNNpvNoc1qtcpqtRZJ/+fPn5fFYlFgYGC+31MiE8AuDaq4OgQgj6CWQ10dAuBg39qprg4BMIXExETFx8c7tI0bN07jx4+/4b4vXryoUaNG6eGHH5a/v3++31ciE0AAAICCcOY2MHFxcYqNjXVoK4rqX3Z2tnr16iXDMDR79uwCvZcEEAAAwImKcrr3iivJ39GjR7Vu3boCVf8kEkAAAICb6qvgriR/Bw4c0Pr161W+fPkC90ECCAAA4EYyMjJ08OBB++u0tDSlpqaqXLlyqlKlih588EHt2LFDn376qXJycnTixAlJUrly5eTl5ZWvMUgAAQCA6bnTV8Ft27ZNUVFR9tdX1g/GxMRo/PjxWrlypSSpadOmDu9bv3692rVrl68xSAABAIDpuVH+p3bt2skwjGuev965/OKbQAAAAEyGCiAAADA9d5oCLg5UAAEAAEyGCiAAADA9KoAAAAAo0agAAgAA0zNZAZAKIAAAgNlQAQQAAKZntjWAJIAAAMD0TJb/MQUMAABgNlQAAQCA6ZltCpgKIAAAgMlQAQQAAKZnsgIgFUAAAACzoQIIAABMz8NkJUAqgAAAACZDBRAAAJieyQqAJIAAAABsAwMAAIASjQogAAAwPQ9zFQCpAAIAAJgNFUAAAGB6rAEEAABAiUYFEAAAmJ7JCoBUAAEAAMyGCiAAADA9i8xVAiQBBAAApsc2MAAAACjRqAACAADTYxsYAAAAlGhUAAEAgOmZrABIBRAAAMBsqAACAADT8zBZCZAKIAAAgMlQAQQAAKZnsgIgCSAAAADbwAAAAKBEowIIAABMz2QFQCqAAAAAZkMFEAAAmB7bwAAAAMBlUlJS1LVrV4WEhMhisWjFihUO5w3D0NixY1WlShX5+PgoOjpaBw4cKNAYJIAAAMD0LE48CiozM1NNmjTRG2+8cdXzkydP1syZM5WcnKytW7fK19dXHTp00MWLF/M9BlPAAAAAbqRTp07q1KnTVc8ZhqGkpCS9/PLL6tatmyTp3XffVeXKlbVixQr17t07X2NQAQQAAKZnsVicdthsNqWnpzscNputUHGmpaXpxIkTio6OtrcFBATotttu05YtW/LdDwkgAAAwPQ+L847ExEQFBAQ4HImJiYWK88SJE5KkypUrO7RXrlzZfi4/mAIGAABwori4OMXGxjq0Wa1WF0XzJxJAAABges78Kjir1VpkCV9wcLAk6ffff1eVKlXs7b///ruaNm2a736YAgYAALhJREREKDg4WGvXrrW3paena+vWrWrdunW++6ECCAAATM+d9oHOyMjQwYMH7a/T0tKUmpqqcuXKKTQ0VMOHD9crr7yi2rVrKyIiQmPGjFFISIi6d++e7zFIAAEAANzItm3bFBUVZX99Zf1gTEyMFixYoBdeeEGZmZl68sknde7cOd15551atWqVvL298z2GxTAMo8gjd7GLl10dAZBXUMuhrg4BcLBv7VRXhwA4CC3nugcjHl2yy2l9v/tIY6f1XVisAQQAADAZlyaAly9f1rvvvqvff//dlWEAAACTc+Y+gO7IpQlgqVKlNGjQoAJ9dx0AAEBRc+Y3gbgjl08Bt2rVSqmpqa4OAwAAwDRc/hTw4MGDFRsbq59//lnNmzeXr6+vw/nGjd1v4SQAAChZ3LNO5zwuTwB79+4tSRo2bJi9zWKxyDAMWSwW5eTkuCo0AACAEqlQCeBXX32lOXPm6NChQ/rwww9VtWpVLVq0SBEREbrzzjsL1FdaWlphQgAAACgyHm66Vs9ZCpwAfvTRR+rXr5/69Omj77//XjabTZJ0/vx5TZo0SZ9//nmB+gsLCytoCAAAALgBBX4I5JVXXlFycrLmzp2r0qVL29vbtGmjHTt2FCqIRYsWqU2bNgoJCdHRo0clSUlJSfr4448L1R8AAEBBWCzOO9xRgRPAffv2qW3btnnaAwICdO7cuQIHMHv2bMXGxuree+/VuXPn7Gv+AgMDlZSUVOD+AAAAcH0FTgCDg4MdvqD4ik2bNqlGjRoFDmDWrFmaO3euXnrpJXl6etrbW7Rood27dxe4PwAAgIJiH8B/8MQTT+jZZ5/V1q1bZbFY9Ntvv2nx4sV6/vnn9fTTTxc4gLS0NN1666152q1WqzIzMwvcHwAAAK6vwA+BjB49Wrm5ubr77ruVlZWltm3bymq16vnnn9czzzxT4AAiIiKUmpqa52GQVatWqV69egXuDwAAoKDctFDnNAVOAC0Wi1566SWNHDlSBw8eVEZGhurXr6+yZcsWKoDY2FgNGTJEFy9elGEY+vbbb/X+++8rMTFR8+bNK1SfuHH/XrJYC+e/rT/+OKVb6tTV6BfHqBGbcqOYtGlWUyMejVaz+qGqUjFAvUa8pU827LKf9/Xx0ivDuqlrVGOVC/DVkd9O6833N2reh5tcGDXM5P2F87Rp41r9fDRNVqtV9Rs11eODh6t6WISrQ0MhsQ1MPnl5eal+/fo3HMDjjz8uHx8fvfzyy8rKytIjjzyikJAQzZgxw75JNIrXqv9+rimTE/XyuHg1atREixct1NNPPaaPP12l8uXLuzo8mICvj1W79/+qdz/eoqXTnsxz/rXnHlC7lrdowEvv6uhvpxXdup5mxPXS8VPn9dlG1g7D+XZ9v033PdBbdeo1UE5Ojt5JnqnRwwdp3pLl8vEp4+rwgH9U4AQwKirqugsa161bV+Ag+vTpoz59+igrK0sZGRmqVKlSgftA0Vm0cL56PNhL3e9/QJL08rh4paRs0IplH+mxJ/L+ZQwUtdVf79Hqr/dc8/ztTSL03qdb9dX2A5Kkd5Z9rcceaKMWDcJIAFEsEpOSHV6PfHmCet7bTgd+2qPGt7ZwUVS4ESYrABb8IZCmTZuqSZMm9qN+/fq6dOmSduzYoUaNGt1QMGXKlCH5c7HsS5e0d8+Pur31HfY2Dw8P3X77Hdq183sXRgb8P9/sTFOXyEYKqRggSWrborZqh1XSl9/sdXFkMKvMjAxJkp9/gIsjAfKnwBXA6dOnX7V9/Pjxyvj//w9QEL///ruef/55rV27VidPnpRhGA7n+S7g4nX23Fnl5OTkmeotX7680tIOuygqwFHsax/ojTEP69DqicrOzlGukavBE97X1zsOuTo0mFBubq5mJ01Wg8a3KqJmbVeHg0Jy1+1anKXQawD/rm/fvmrVqpWmTJlSoPf1799fx44d05gxY1SlSpUC/w9gs9nsX0d3heFpldVqLVA/AG4eg3tHqlWjcD3wbLKOHT+jO5vVUtLoP9cArt+6z9XhwWRmTZmoI4cPavqcBa4OBci3IksAt2zZIm9v7wK/b9OmTfrqq6/UtGnTQo2bmJio+Ph4h7aXxozTy2PHF6o/swsKDJKnp6dOnz7t0H769GlVqFDBRVEB/4+3tbTin+mqh2LnatWmHyVJPxz4TY3rVNPwfneTAKJYzZoySVu/TtHU2fNVsVKwq8PBDSjwmribXIETwB49eji8NgxDx48f17Zt2zRmzJgCB1C9evU8074FERcXp9jYWMeYPKn+FVZpLy/Vq99AW7/Zov+7O1rSn9MbW7duUe+H+7o4OkAqXcpTXqVLKTfPcpFceXiYawoHrmMYhl6fmqivN67TlDffVpWQaq4OCSiQAieAAQGOC1w9PDxUp04dJSQkqH379gUOICkpSaNHj9acOXMUHh5e4PdbrXmney9eLnA3+It+MQM05sVRatCgoRo2aqz3Fi3UhQsX1P3+Hv/8ZqAI+Pp4qWb1ivbX4VXLq/EtVXU2PUs/nzirlG0HNGl4d124mK1jx8/orua11KdLK42atsyFUcNMZk2ZqHWr/6v412aoTBlfnTn9hyTJ17esrIWYDYPrmW0NoMUoQPktJydHX3/9tRo1aqSgoKBCDxoUFOTwg87MzNTly5dVpkwZlS5d2uHaM2fOFLh/EsAb9/7i9+wbQdepW0+jXnxZjRs3cXVYN7WglkNdHcJN467mtbV63rN52het/EZPjntPlcv7KeGZbopuXVdB/mV07PgZvbNss2a+V/BtqMxs39qprg7hpnVP66tvjP/8yxPUoXO3Yo6m5Agt57oZvOEf/+S0vpO61XVa34VVoARQkry9vbV3715FRBR+t/OFCxfm+9qYmJgC908CCHdEAgh3QwIId0MCWHwKPAXcsGFDHT58+IYSwMIkdQAAAM5itiXEBX7o5ZVXXtHzzz+vTz/9VMePH1d6errDUVCenp46efJknvbTp0/L09OzwP0BAADg+vJdAUxISNBzzz2ne++9V5J03333OazjMwxDFoulwBs3X2sG2mazycvLq0B9AQAAFIbZHgLJdwIYHx+vQYMGaf369UUy8MyZMyX9+QOfN2+eypYtaz+Xk5OjlJQU1a3rfnPmAAAAN7t8J4BXKnWRkZFFMvCVr5QzDEPJyckO071eXl4KDw9XcnLytd4OAABQZMy2BrBAD4EUZXk0LS1NkhQVFaVly5bp8uXLslgsfNsEAACAkxUoAbzlllv+MQksyL59586dU7169VS7dm2dPXtW0p97BPbu3VuvvPKKAgMDCxIeAABAoZhsCWDBEsD4+Pg83wRSWGfOnFHr1q3166+/qk+fPqpXr54kac+ePVqwYIHWrl2rzZs339CG0wAAAPnhYbIMsEAJYO/evVWpUqUiGTghIUFeXl46dOiQKleunOdc+/btlZCQYF8rCAAAgKKR730Ai/rx6BUrVmjKlCl5kj9JCg4O1uTJk7V8+fIiHRMAAOBqPJx4uKN8x1XAb4z7R8ePH1eDBg2ueb5hw4Y6ceJEkY4JAACAAkwB5+bmFunAFSpU0JEjR1StWrWrnk9LS1O5cuWKdEwAAICrMdkSQNdVJjt06KCXXnpJly5dynPOZrNpzJgx6tixowsiAwAAKNkK9BBIUUpISFCLFi1Uu3ZtDRkyRHXr1pVhGNq7d6/efPNN2Ww2LVq0yFXhAQAAE+Ep4GJSrVo1bdmyRYMHD1ZcXJx9jaHFYtE999yj119/XdWrV3dVeAAAACWWyxJASYqIiNB///tfnT17VgcOHJAk1apVi7V/AACgWJmsAOgeTycHBQWpVatWatWqFckfAAAodh4W5x0FkZOTozFjxigiIkI+Pj6qWbOmJkyYUOS7sbi0AggAAID/57XXXtPs2bO1cOFCNWjQQNu2bdOAAQMUEBCgYcOGFdk4JIAAAMD03OUhkM2bN6tbt27q3LmzJCk8PFzvv/++vv322yIdxy2mgAEAAEoqm82m9PR0h8Nms1312jvuuENr167V/v37JUk7d+7Upk2b1KlTpyKNiQQQAACYnsXivCMxMVEBAQEOR2Ji4lXjGD16tHr37q26deuqdOnSuvXWWzV8+HD16dOnSO+XKWAAAAAniouLU2xsrEOb1Wq96rX/+c9/tHjxYi1ZskQNGjRQamqqhg8frpCQEMXExBRZTCSAAADA9Ar6tG5BWK3WayZ8fzdy5Eh7FVCSGjVqpKNHjyoxMbFIE0CmgAEAANxEVlaWPDwc0zNPT0/l5uYW6ThUAAEAgOlZ5B5PAXft2lUTJ05UaGioGjRooO+//17Tpk3TwIEDi3QcEkAAAGB6zpwCLohZs2ZpzJgxGjx4sE6ePKmQkBA99dRTGjt2bJGOQwIIAADgJvz8/JSUlKSkpCSnjkMCCAAATM9dKoDFhYdAAAAATIYKIAAAMD2Lm3wVXHGhAggAAGAyVAABAIDpsQYQAAAAJRoVQAAAYHomWwJIAggAAOBhsgyQKWAAAACToQIIAABMj4dAAAAAUKJRAQQAAKZnsiWAVAABAADMhgogAAAwPQ+ZqwRIBRAAAMBkqAACAADTM9saQBJAAABgemwDAwAAgBKNCiAAADA9vgoOAAAAJRoVQAAAYHomKwBSAQQAADAbKoAAAMD0WAMIAACAEo0KIAAAMD2TFQBJAAEAAMw2JWq2+wUAADA9KoAAAMD0LCabA6YCCAAAYDJUAAEAgOmZq/5HBRAAAMB0qAACAADTYyNoAAAAlGhUAAEAgOmZq/5HAggAAGC6bwJhChgAAMBkqAACAADTYyNoAAAAlGhUAAEAgOmZrSJmtvsFAAAwPSqAAADA9FgDCAAAAJf59ddf1bdvX5UvX14+Pj5q1KiRtm3bVqRjUAEEAACm5y71v7Nnz6pNmzaKiorSf//7X1WsWFEHDhxQUFBQkY5DAggAAOAmXnvtNVWvXl3z58+3t0VERBT5OEwBAwAA07NYLE47bDab0tPTHQ6bzXbVOFauXKkWLVqoZ8+eqlSpkm699VbNnTu36O/XMAyjyHt1se1H0l0dApBHGaunq0MAHDww62tXhwA42DOpvcvGXrbzuNP63rV8juLj4x3axo0bp/Hjx+e51tvbW5IUGxurnj176rvvvtOzzz6r5ORkxcTEFFlMJIBAMSEBhLshAYS7KakJYOe65fJU/KxWq6xWa55rvby81KJFC23evNneNmzYMH333XfasmVLkcXEGkAAAGB6ztwG5lrJ3tVUqVJF9evXd2irV6+ePvrooyKNiTWAAAAAbqJNmzbat2+fQ9v+/fsVFhZWpOOQAAIAANOzOPEoiBEjRuibb77RpEmTdPDgQS1ZskRvvfWWhgwZcoN36IgEEAAAwE20bNlSy5cv1/vvv6+GDRtqwoQJSkpKUp8+fYp0HNYAAgAA03Onb4Lr0qWLunTp4tQxqAACAACYDBVAAABgeh5u82VwxYMEEAAAmJ47TQEXB6aAAQAATIYKIAAAMD2LyaaAqQACAACYDBVAAABgeqwBBAAAQIlGBRAAAJie2baBoQIIAABgMlQAAQCA6ZltDSAJIAAAMD2zJYBMAQMAAJgMFUAAAGB6bAQNAACAEo0KIAAAMD0PcxUAqQACAACYDRVAAABgeqwBBAAAQIlGBRAAAJie2fYBJAEEAACmxxQwAAAASjQqgAAAwPTYBgYAAAAlGhVAAABgeqwBBAAAQIlGBRAAAJie2baBoQIIAABgMlQAAQCA6ZmsAEgCCAAA4GGyOWCmgAEAAEyGCiAAADA9c9X/qAACAACYDhVAAAAAk5UAqQACAACYDBVAAABgenwVHAAAAEo0KoAAAMD0TLYNIAkgAACAyfI/poABAADMhgQQAADA4sTjBrz66quyWCwaPnz4jXX0NySAAAAAbui7777TnDlz1Lhx4yLvmwQQAACYnsWJ/xVGRkaG+vTpo7lz5yooKKiI75YEEAAAwKlsNpvS09MdDpvNdt33DBkyRJ07d1Z0dLRTYiIBBAAApmexOO9ITExUQECAw5GYmHjNWP79739rx44d173mRrENDAAAgBPFxcUpNjbWoc1qtV712p9//lnPPvus1qxZI29vb6fFRAIIAABMz5n7AFqt1msmfH+3fft2nTx5Us2aNbO35eTkKCUlRa+//rpsNps8PT1vOCYSQAAAADfZCfruu+/W7t27HdoGDBigunXratSoUUWS/EkkgAAAAG7Dz89PDRs2dGjz9fVV+fLl87TfCBJAAABgeoXdruVmRQIIAADgxjZs2FDkfZIAAgAA07OYqwDIPoAAAABmQwUQAACYnskKgFQAAQAAzIYKIAAAgMlKgCSAAADA9My2DQxTwAAAACZDBRAAAJge28C4gKenp06ePJmn/fTp00X2nXcAAAD4k1tUAA3DuGq7zWaTl5dXMUcDAADMxmQFQNcmgDNnzpQkWSwWzZs3T2XLlrWfy8nJUUpKiurWreuq8AAAAEoklyaA06dPl/RnBTA5OdlhutfLy0vh4eFKTk52VXgAAMAsTFYCdGkCmJaWJkmKiorSsmXLFBQU5MpwAAAATMEt1gCuX7/e4XVOTo52796tsLAwkkIXW7l0gf79zhvq2L23Hn36OVeHA5P678cfaNXHH+jkieOSpNDwGuoV86Sa39bGxZHBLJqHB2ngXeFqUNVPlfy99cyi77V27yn7+SF311SnxsEKDvBWdk6u9vyarhmrD2rXL+ddGDUKgn0AXWD48OF6++23Jf2Z/LVt21bNmjVT9erVtWHDBtcGZ2KH9v2otZ8tV2hEbVeHApMrX7GS+j05TFPfWqwpc95To2YtlfjSCB1LO+Tq0GASZbw8te/E/zRh5U9XPX/kj0xNXLlX3WdsVr853+rXsxc0d2AzBfmWLuZIgfxxiwTwgw8+UJMmTSRJn3zyiY4cOaKffvpJI0aM0EsvveTi6Mzp4oUsvfHaWD0+/EX5+vm5OhyYXKs7ItXi9jsVUi1UVauHqe/jQ+XtU0b79ux2dWgwia/2/6GZaw5q7Z68W5ZJ0mc7T2jLoTP65ewFHTyZqdc+3yc/79KqE8zvz5uFxeK8wx25RQJ4+vRpBQcHS5I+//xz9ezZU7fccosGDhyo3bv5Be8K81+frFtbtVGjZre5OhTAQU5Ojr5a+4UuXrygug0auzocII/Snhb1allN6Rey9dPx/7k6HOSTxYmHO3KLNYCVK1fWnj17VKVKFa1atUqzZ8+WJGVlZbERtAts3rBaRw7+pAmzFro6FMDuyOEDGj24vy5duiRvHx+NnjBV1cNruDoswC6yTgVN7d1Y3qU9dep/Nj3+znady8p2dVjAVblFAjhgwAD16tVLVapUkcViUXR0tCRp69at/7gPoM1mk81mc2i7ZLPJy2p1Wrwl2emTJ/Tu7Kl6MfF1eXnxM4T7qFo9XNPnva/MzAxt2bhWMxPHauKMeSSBcBvfHj6rHrO2KNDXSz1bVtW0h5uo9+ytOpN5ydWhIT/ctVTnJG4xBTx+/HjNmzdPTz75pL7++mtZ///kzdPTU6NHj77uexMTExUQEOBwzJ89rTjCLpEOH/xJ6efO6MUh/dS30+3q2+l27d21Q198vFR9O92u3JwcV4cIkypdurSqVAtVrTr11e/JZxRe8xZ98tESV4cF2F3IztGxMxe06+fzGrNsj3Jyc/VAi6quDgu4KreoAErSgw8+mKctJibmH98XFxen2NhYh7Yfj9uucTX+ScOmLfXanPcd2uZMTVBI9XB17fWoPJiSh5swjFxlX2J6De7LYrHIq5Rb1FmQD2bbBsYtEsCEhITrnh87duw1z1mtVnvF8AqvM+lFEpcZ+ZTxVfXwWg5tVm8flfULyNMOFJdFb81Ss9vuUIVKVXThQqa++nKVfkjdrnH/esPVocEkynh5KrR8GfvrquV8VLeKn85nZetcVraeiorQur2n9Mf/bAosU1qP3B6qyv5WfbH7hAujBq7NLRLA5cuXO7zOzs5WWlqaSpUqpZo1a143AQRQ8p07d0ZJk8bq7Jk/5OtbVmE1amvcv95Q0xa3uzo0mESDqv5a+ERL++vRnf9cn758+6+K/3ivIir6asatIQry9dK5rEv64Zd09XvrOx08memqkFFA7rpdi7NYDMMwXB3E1aSnp6t///66//771a9fvwK9d/sRKoBwP2WsTJ/DvTww62tXhwA42DOpvcvG3nciy2l91wku888XFTO3XZzg7++v+Ph4jRkzxtWhAACAEo59AN3I+fPndf4836MIAACczF0zNSdxiwRw5syZDq8Nw9Dx48e1aNEidezY0UVRAQAAlExukQBOnz7d4bWHh4cqVqyomJgYxcXFuSgqAABgFmwD4wJpaWl52i5evKg33nhDtWvX1okTPEYPAABQVFz6EIjNZlNcXJxatGihNm3aaMWKFZKk+fPnq2bNmpoxY4ZGjBjhyhABAIAJWCzOO9yRSyuAY8eO1Zw5cxQdHa3NmzerZ8+eGjBggL755htNnTpVPXv2lCffPAEAAFCkXJoAfvDBB3r33Xd133336YcfflDjxo11+fJl7dy5UxZ3TZkBAECJY7asw6VTwL/88ouaN28uSWrYsKGsVqtGjBhB8gcAAOBELq0A5uTkyMvLy/66VKlSKlu2rAsjAgAApmSy2pNLE0DDMNS/f39ZrVZJfz75O2jQIPn6+jpct2zZMleEBwAATIJtYIpRTEyMw+u+ffu6KBIAAADzcGkCOH/+fFcODwAAIMl9t2txFpc+BAIAAIDi5xbfBAIAAOBKJisAUgEEAAAwGxJAAAAAixOPAkhMTFTLli3l5+enSpUqqXv37tq3b9+N3l0eJIAAAABuYuPGjRoyZIi++eYbrVmzRtnZ2Wrfvr0yMzOLdBzWAAIAANNzl30AV61a5fB6wYIFqlSpkrZv3662bdsW2TgkgAAAwPScuQ2MzWaTzWZzaLNarfYvwrie8+fPS5LKlStXpDExBQwAAOBEiYmJCggIcDgSExP/8X25ubkaPny42rRpo4YNGxZpTFQAAQCA6TlzAjguLk6xsbEObfmp/g0ZMkQ//PCDNm3aVOQxkQACAAA4UX6ne/9q6NCh+vTTT5WSkqJq1aoVeUwkgAAAwPTc5avgDMPQM888o+XLl2vDhg2KiIhwyjgkgAAAAG5iyJAhWrJkiT7++GP5+fnpxIkTkqSAgAD5+PgU2Tg8BAIAAOAmO0HPnj1b58+fV7t27VSlShX7sXTp0hu+w7+iAggAAOAmDMMolnFIAAEAgOm5yxrA4kICCAAATM9k+R9rAAEAAMyGCiAAADA9s00BUwEEAAAwGSqAAADA9CwmWwVIBRAAAMBkqAACAACYqwBIBRAAAMBsqAACAADTM1kBkAQQAACAbWAAAABQolEBBAAApsc2MAAAACjRqAACAACYqwBIBRAAAMBsqAACAADTM1kBkAogAACA2VABBAAApme2fQBJAAEAgOmxDQwAAABKNCqAAADA9Mw2BUwFEAAAwGRIAAEAAEyGBBAAAMBkWAMIAABMjzWAAAAAKNGoAAIAANMz2z6AJIAAAMD0mAIGAABAiUYFEAAAmJ7JCoBUAAEAAMyGCiAAAIDJSoBUAAEAAEyGCiAAADA9s20DQwUQAADAZKgAAgAA02MfQAAAAJRoVAABAIDpmawASAIIAABgtgyQKWAAAACTIQEEAACmZ3Hif4XxxhtvKDw8XN7e3rrtttv07bffFun9kgACAAC4kaVLlyo2Nlbjxo3Tjh071KRJE3Xo0EEnT54ssjFIAAEAgOlZLM47CmratGl64oknNGDAANWvX1/JyckqU6aM3nnnnSK7XxJAAAAAJ7LZbEpPT3c4bDbbVa+9dOmStm/frujoaHubh4eHoqOjtWXLliKLqUQ+Bdw83N/VIZQINptNiYmJiouLk9VqdXU4AJ/JIrZnUntXh1Ai8LksGbydmBGNfyVR8fHxDm3jxo3T+PHj81z7xx9/KCcnR5UrV3Zor1y5sn766acii8liGIZRZL2hRElPT1dAQIDOnz8vf3+Sargen0m4Iz6X+Cc2my1Pxc9qtV71Hwy//fabqlatqs2bN6t169b29hdeeEEbN27U1q1biySmElkBBAAAcBfXSvaupkKFCvL09NTvv//u0P77778rODi4yGJiDSAAAICb8PLyUvPmzbV27Vp7W25urtauXetQEbxRVAABAADcSGxsrGJiYtSiRQu1atVKSUlJyszM1IABA4psDBJAXJPVatW4ceNY1Ay3wWcS7ojPJYraQw89pFOnTmns2LE6ceKEmjZtqlWrVuV5MORG8BAIAACAybAGEAAAwGRIAAEAAEyGBBAAAMBkSABRYBs2bJDFYtG5c+eue114eLiSkpKKJSaUbAsWLFBgYKCrwwCAEoMEsATp37+/unfvnqc9vwlbYfGXM/Lr559/1sCBAxUSEiIvLy+FhYXp2Wef1enTp+3X8A8HuFL//v1lsVhksVhUunRpRURE6IUXXtDFixeLpH8+33AXJIAAisXhw4fVokULHThwQO+//74OHjyo5ORk++amZ86cKfaYsrOzi31MuL+OHTvq+PHjOnz4sKZPn645c+Zo3Lhxrg4LKFIkgCa0adMm3XXXXfLx8VH16tU1bNgwZWZm2s8vWrRILVq0kJ+fn4KDg/XII4/o5MmTV+1rw4YNGjBggM6fP2//V/Nfv9w6KytLAwcOlJ+fn0JDQ/XWW285+/bgpoYMGSIvLy+tXr1akZGRCg0NVadOnfTll1/q119/1UsvvaR27drp6NGjGjFihP3z9FdffPGF6tWrp7Jly9r/kv6refPmqV69evL29lbdunX15ptv2s8dOXJEFotFS5cuVWRkpLy9vbV48eJiuXfcXKxWq4KDg1W9enV1795d0dHRWrNmjaSrV/CaNm1q/71nGIbGjx+v0NBQWa1WhYSEaNiwYZL0j59voDiRAJrMoUOH1LFjRz3wwAPatWuXli5dqk2bNmno0KH2a7KzszVhwgTt3LlTK1as0JEjR9S/f/+r9nfHHXcoKSlJ/v7+On78uI4fP67nn3/efn7q1Klq0aKFvv/+ew0ePFhPP/209u3b5+zbhJs5c+aMvvjiCw0ePFg+Pj4O54KDg9WnTx8tXbpUH330kapVq6aEhAT75+mKrKwsTZkyRYsWLVJKSoqOHTvm8FlbvHixxo4dq4kTJ2rv3r2aNGmSxowZo4ULFzqMN3r0aD377LPau3evOnTo4Nwbx03vhx9+0ObNm+Xl5ZWv6z/66CN71fDAgQNasWKFGjVqJElatmzZNT/fQHHjm0BKmE8//VRly5Z1aMvJybH/OTExUX369NHw4cMlSbVr19bMmTMVGRmp2bNny9vbWwMHDrRfX6NGDc2cOVMtW7ZURkZGnr69vLwUEBAgi8Vy1S+pvvfeezV48GBJ0qhRozR9+nStX79ederUKapbxk3gwIEDMgxD9erVu+r5evXq6ezZs8rJyZGnp6e9+vxX2dnZSk5OVs2aNSVJQ4cOVUJCgv38uHHjNHXqVPXo0UOSFBERoT179mjOnDmKiYmxXzd8+HD7NcDVXPk9evnyZdlsNnl4eOj111/P13uPHTum4OBgRUdHq3Tp0goNDVWrVq0kSeXKlbvm5xsobiSAJUxUVJRmz57t0LZ161b17dtXkrRz507t2rXLYerLMAzl5uYqLS1N9erV0/bt2zV+/Hjt3LlTZ8+eVW5urqQ/f7HVr1+/QPE0btzY/ucrSeK1ppNR8t3IFw+VKVPGnvxJUpUqVeyfpczMTB06dEiPPfaYnnjiCfs1ly9fVkBAgEM/LVq0KHQMMIcrv0czMzM1ffp0lSpVSg888EC+3tuzZ08lJSWpRo0a6tixo+6991517dpVpUrx1y3cC5/IEsbX11e1atVyaPvll1/sf87IyNBTTz1lX5PyV6GhocrMzFSHDh3UoUMHLV68WBUrVtSxY8fUoUMHXbp0qcDxlC5d2uG1xWKxJ5Qwj1q1aslisWjv3r26//7785zfu3evgoKCVLFixWv2cbXP0pWEMiMjQ5I0d+5c3XbbbQ7XeXp6Orz29fUt1D3APP76e/Sdd95RkyZN9Pbbb+uxxx6Th4dHnn/I/PVhourVq2vfvn368ssvtWbNGg0ePFj/+te/tHHjxjyfYcCVSABNplmzZtqzZ0+eJPGK3bt36/Tp03r11VdVvXp1SdK2bduu26eXl5fDNDPwd+XLl9c999yjN998UyNGjHBYB3jixAktXrxYjz76qCwWS6E+T5UrV1ZISIgOHz6sPn36FHX4MDEPDw+9+OKLio2N1SOPPKKKFSs6rN1LT09XWlqaw3t8fHzUtWtXde3aVUOGDFHdunW1e/duNWvWjN+XcBs8BGIyo0aN0ubNmzV06FClpqbqwIED+vjjj+0PgYSGhsrLy0uzZs3S4cOHtXLlSk2YMOG6fYaHhysjI0Nr167VH3/8oaysrOK4FdxkXn/9ddlsNnXo0EEpKSn6+eeftWrVKt1zzz2qWrWqJk6cKOnPz1NKSop+/fVX/fHHH/nuPz4+XomJiZo5c6b279+v3bt3a/78+Zo2bZqzbgkm0bNnT3l6euqNN97Q//3f/2nRokX66quvtHv3bsXExDhUmRcsWKC3335bP/zwgw4fPqz33ntPPj4+CgsLk1T4zzdQ1EgATaZx48bauHGj9u/fr7vuuku33nqrxo4dq5CQEElSxYoVtWDBAn3wwQeqX7++Xn31VU2ZMuW6fd5xxx0aNGiQHnroIVWsWFGTJ08ujlvBTaZ27dratm2batSooV69eqlmzZp68sknFRUVpS1btqhcuXKSpISEBB05ckQ1a9a87pTw3z3++OOaN2+e5s+fr0aNGikyMlILFixQRESEs24JJlGqVCkNHTpUkydP1ujRoxUZGakuXbqoc+fO6t69u8Pa1MDAQM2dO1dt2rRR48aN9eWXX+qTTz5R+fLlJRX+8w0UNYtxI6uyAQAAcNOhAggAAGAyJIAAAAAmQwIIAABgMiSAAAAAJkMCCAAAYDIkgAAAACZDAggAAGAyJIAAAAAmQwIIwG31799f3bt3t79u166dhg8fXuxxbNiwQRaLRefOnSv2sQHAGUgAARRY//79ZbFYZLFY5OXlpVq1aikhIUGXL1926rjLli37x++mvoKkDQCurZSrAwBwc+rYsaPmz58vm82mzz//XEOGDFHp0qUVFxfncN2lS5fk5eVVJGNe+b5gAMCNoQIIoFCsVquCg4MVFhamp59+WtHR0Vq5cqV92nbixIkKCQlRnTp1JEk///yzevXqpcDAQJUrV07dunXTkSNH7P3l5OQoNjZWgYGBKl++vF544QX9/avK/z4FbLPZNGrUKFWvXl1Wq1W1atXS22+/rSNHjigqKkqSFBQUJIvFov79+0uScnNzlZiYqIiICPn4+KhJkyb68MMPHcb5/PPPdcstt8jHx0dRUVEOcQJASUACCKBI+Pj46NKlS5KktWvXat++fVqzZo0+/fRTZWdnq0OHDvLz89NXX32lr7/+WmXLllXHjh3t75k6daoWLFigd955R5s2bdKZM2e0fPny64756KOP6v3339fMmTO1d+9ezZkzR2XLllX16tX10UcfSZL27dun48ePa8aMGZKkxMREvfvuu0pOTtaPP/6oESNGqG/fvtq4caOkPxPVHj16qGvXrkpNTdXjjz+u0aNHO+vHBgAuwRQwgBtiGIbWrl2rL774Qs8884xOnTolX19fzZs3zz71+9577yk3N1fz5s2TxWKRJM2fP1+BgYHasGGD2rdvr6SkJMXFxalHjx6SpOTkZH3xxRfXHHf//v36z3/+ozVr1ig6OlqSVKNGDfv5K9PFlSpVUmBgoKQ/K4aTJk3Sl19+qdatW9vfs2nTJs2ZM0eRkZGaPXu2atasqalTp0qS6tSpo927d+u1114rwp8aALgWCSCAQvn0009VtmxZZWdnKzc3V4888ojGjx+vIUOGqFGjRg7r/nbu3KmDBw/Kz8/PoY+LFy/q0KFDOn/+vI4fP67bbrvNfq5UqVJq0aJFnmngK1JTU+Xp6anIyMh8x3zw4EFlZWXpnnvucWi/dOmSbr31VknS3r17HeKQZE8WAaCkIAEEUChRUVGaPXu2vLy8FBISolKl/t+vE19fX4drMzIy1Lx5cy1evDhPPxUrVizU+D4+PgV+T0ZGhiTps88+U9WqVR3OWa3WQsUBADcjEkAAheLr66tatWrl69pmzZpp6dKlqlSpkvz9/a96TZUqVbR161a1bdtWknT58mVt375dzZo1u+r1jRo1Um5urjZu3GifAv6rKxXInJwce1v9+vVltVp17Nixa1YO69Wrp5UrVzq0ffPNN/98kwBwE+EhEABO16dPH1WoUEHdunXTV199pbS0NG3YsEHDhg3TL7/8Ikl69tln9eqrr2rFihX66aefNHjw4Ovu4RceHq6YmBgNHDhQK1assPf5n//8R5IUFhYmi8WiTz/9VKdOnVJGRob8/Pz0/PPPa8SIEVq4cKEOHTqkHTt2aNasWVq4cKEkadCgQTpw4IBGjhypffv2acmSJVqwYIGzf0QAUKxIAAE4XZkyZZSSkqLQ0FD16NFD9erV02OPPaaLFy/aK4LPPfec+vXrp5iYGLVu3Vp+fn66//77r9vv7Nmz9eCDD2rw4MGqW7eunnjiCWVmZkqSqlatqvj4eI0ePVqVK1fW0KFDJUkTJkzQmDFjlJiYqHr16qljx4767LPPFBERIUkKDQ3VRx99pBUrVqhJkyZKTk7WpEmTnPjTAYDiZzGutcIaAAAAJRIVQAAAAJMhAQQAADAZEkAAAACTIQEEAAAwGRJAAAAAkyEBBAAAMBkSQAAAAJMhAQQAADAZEkAAAACTIQEEAAAwGRJAAAAAk/n/AGLNAigrJyvOAAAAAElFTkSuQmCC\n"},"metadata":{}},{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n      Health       0.60      0.30      0.40        20\n       Other       0.67      0.90      0.77        20\n        Rust       0.57      0.65      0.60        20\n\n    accuracy                           0.62        60\n   macro avg       0.61      0.62      0.59        60\nweighted avg       0.61      0.62      0.59        60\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nepochs = range(1, num_epochs + 1)\n\n# Plot accuracy\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, val_accuracies, label='Validation Accuracy', marker='o')\nplt.plot(epochs, [acc for acc in val_accuracies], label='Test Accuracy Snapshot (same as val)', linestyle='--', marker='x')\nplt.plot(epochs, [None if i >= len(val_accuracies) else val_accuracies[i] for i in range(num_epochs)],\n         label='(Simulated) Train Accuracy Line', linestyle=':', color='gray')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:24:47.630696Z","iopub.execute_input":"2025-04-15T18:24:47.631245Z","iopub.status.idle":"2025-04-15T18:24:47.724859Z","shell.execute_reply.started":"2025-04-15T18:24:47.631191Z","shell.execute_reply":"2025-04-15T18:24:47.723186Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2adc5619a08c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"],"ename":"NameError","evalue":"name 'num_epochs' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"# Hybrid-SSRN-ViT Architecture-For-Hyperspectral-Image-Classification","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom glob import glob\nimport cv2\nimport rasterio\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:43:04.877548Z","iopub.execute_input":"2025-04-15T18:43:04.877908Z","iopub.status.idle":"2025-04-15T18:43:06.717748Z","shell.execute_reply.started":"2025-04-15T18:43:04.877875Z","shell.execute_reply":"2025-04-15T18:43:06.716983Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"processed_dataset_path = \"/kaggle/working/processed_images\"  # where processed npy files were saved\nsplit_dataset_path = \"/kaggle/working/split_dataset\"   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:43:14.927735Z","iopub.execute_input":"2025-04-15T18:43:14.928298Z","iopub.status.idle":"2025-04-15T18:43:14.932075Z","shell.execute_reply.started":"2025-04-15T18:43:14.928264Z","shell.execute_reply":"2025-04-15T18:43:14.931209Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport numpy as np\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\n# -------------------------------\n# Custom Transform Definitions\n# -------------------------------\n\nclass TensorResize:\n    \"\"\"Resizes a tensor image to the target size.\"\"\"\n    def __init__(self, size):\n        self.size = size  # e.g. (128, 128) or (224, 224)\n    def __call__(self, img):\n        # img is expected to be a tensor of shape (C, H, W)\n        return torch.nn.functional.interpolate(\n            img.unsqueeze(0), size=self.size, mode='bilinear', align_corners=False\n        ).squeeze(0)\n\nclass RandomHorizontalFlipTensor:\n    \"\"\"Randomly horizontally flips a tensor image with probability p.\"\"\"\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, img):\n        if torch.rand(1).item() < self.p:\n            return torch.flip(img, dims=[-1])\n        return img\n\nclass RandomVerticalFlipTensor:\n    \"\"\"Randomly vertically flips a tensor image with probability p.\"\"\"\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, img):\n        if torch.rand(1).item() < self.p:\n            return torch.flip(img, dims=[-2])\n        return img\n\nclass RandomRotationTensor:\n    \"\"\"Randomly rotates a tensor image by an angle between -max_angle and max_angle degrees.\"\"\"\n    def __init__(self, max_angle=30):\n        self.max_angle = max_angle\n    def __call__(self, img):\n        angle = torch.empty(1).uniform_(-self.max_angle, self.max_angle).item()\n        return TF.rotate(img, angle)\n\nclass AddGaussianNoise:\n    \"\"\"Adds Gaussian noise to a tensor image.\"\"\"\n    def __init__(self, mean=0.0, std=0.1):\n        self.mean = mean\n        self.std = std\n    def __call__(self, img):\n        noise = torch.randn_like(img) * self.std + self.mean\n        return img + noise\n\nclass RandomContrastTensor:\n    \"\"\"Randomly adjusts contrast for a tensor image by a factor chosen uniformly.\"\"\"\n    def __init__(self, lower=0.7, upper=1.3):\n        self.lower = lower\n        self.upper = upper\n    def __call__(self, img):\n        factor = torch.empty(1).uniform_(self.lower, self.upper).item()\n        mean = torch.mean(img, dim=[1,2], keepdim=True)\n        return (img - mean) * factor + mean\n\n# -------------------------------\n# AgricultureHSIDataset Definition\n# -------------------------------\n\nclass AgricultureHSIDataset(torch.utils.data.Dataset):\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Directory with images in subdirectories per category.\n            transform (callable, optional): Optional transform to be applied on a tensor sample.\n        \"\"\"\n        # If no transform is provided, use the default augmentation pipeline.\n        if transform is None:\n            transform = T.Compose([\n                TensorResize((224, 224)),\n                RandomHorizontalFlipTensor(p=0.5),\n                RandomVerticalFlipTensor(p=0.5),\n                RandomRotationTensor(max_angle=30),\n                AddGaussianNoise(mean=0.0, std=0.05),\n                RandomContrastTensor(lower=0.8, upper=1.2)\n            ])\n        self.transform = transform\n        self.root_dir = root_dir\n        # Build list of files along with labels\n        self.image_paths = []\n        self.labels = []\n        # Mapping for categories to integers; update keys if folder names differ\n        self.label_mapping = {\"Health\": 0, \"Rust\": 1, \"Other\": 2}\n        categories = os.listdir(root_dir)\n        for cat in categories:\n            cat_path = os.path.join(root_dir, cat)\n            if os.path.isdir(cat_path):\n                # Expecting npy files in each category folder.\n                for f in glob.glob(os.path.join(cat_path, \"*.npy\")):\n                    self.image_paths.append(f)\n                    self.labels.append(self.label_mapping[cat])\n                    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load the processed npy file. Each image is expected to be stored as (bands, H, W)\n        image = np.load(self.image_paths[idx]).astype(np.float32)\n        label = self.labels[idx]\n        # Convert numpy array to torch tensor\n        image = torch.tensor(image)\n        # Apply transformations if available\n        if self.transform:\n            image = self.transform(image)\n        return image, torch.tensor(label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:50:38.887283Z","iopub.execute_input":"2025-04-15T18:50:38.887569Z","iopub.status.idle":"2025-04-15T18:50:38.901715Z","shell.execute_reply.started":"2025-04-15T18:50:38.887549Z","shell.execute_reply":"2025-04-15T18:50:38.900796Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"batch_size = 16\n\ntrain_dataset = AgricultureHSIDataset(os.path.join(split_dataset_path, \"train\"))\nval_dataset   = AgricultureHSIDataset(os.path.join(split_dataset_path, \"val\"))\ntest_dataset  = AgricultureHSIDataset(os.path.join(split_dataset_path, \"test\"))\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\nprint(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:50:42.382504Z","iopub.execute_input":"2025-04-15T18:50:42.382816Z","iopub.status.idle":"2025-04-15T18:50:42.393548Z","shell.execute_reply.started":"2025-04-15T18:50:42.382792Z","shell.execute_reply":"2025-04-15T18:50:42.392532Z"}},"outputs":[{"name":"stdout","text":"Train samples: 480, Val samples: 60, Test samples: 60\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"SELECTED_BANDS = 100  # Ensure this matches the band selection used during processing\n\nclass SSRN(nn.Module):\n    def __init__(self, in_channels, num_features=64):\n        super(SSRN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, num_features, kernel_size=3, padding=1)\n        self.bn1   = nn.BatchNorm2d(num_features)\n        self.relu  = nn.ReLU()\n        # Residual block\n        self.res_block = nn.Sequential(\n            nn.Conv2d(num_features, num_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_features),\n            nn.ReLU(),\n            nn.Conv2d(num_features, num_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_features)\n        )\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        res = self.res_block(out)\n        out = self.relu(out + res)\n        return out\n\nclass ViTHead(nn.Module):\n    def __init__(self, in_features, num_classes, embed_dim=256, num_heads=4, num_layers=2):\n        super(ViTHead, self).__init__()\n        # A convolutional patch embedding layer\n        self.patch_embed = nn.Conv2d(in_features, embed_dim, kernel_size=3, stride=2, padding=1)\n        # Transformer encoder block\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        # x shape: (batch_size, channels, H, W)\n        x = self.patch_embed(x)   # Now: (batch_size, embed_dim, H', W')\n        batch_size, embed_dim, H_new, W_new = x.shape\n        # Flatten spatial dimensions into patches\n        x = x.flatten(2).transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n        x = self.transformer_encoder(x)\n        # Aggregate features across patches (simple average)\n        x = x.mean(dim=1)\n        logits = self.classifier(x)\n        return logits\n\n# Full hybrid SSRN-ViT model combining the spectral-spatial extractor and ViT head.\nclass SSRN_ViT(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(SSRN_ViT, self).__init__()\n        self.ssrn = SSRN(in_channels)\n        # The SSRN is configured to output 64 channels.\n        self.vit_head = ViTHead(in_features=64, num_classes=num_classes)\n\n    def forward(self, x):\n        features = self.ssrn(x)\n        logits = self.vit_head(features)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:54:12.492918Z","iopub.execute_input":"2025-04-15T18:54:12.493370Z","iopub.status.idle":"2025-04-15T18:54:12.505337Z","shell.execute_reply.started":"2025-04-15T18:54:12.493333Z","shell.execute_reply":"2025-04-15T18:54:12.504217Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\nprint(device)\nmodel = SSRN_ViT(in_channels=SELECTED_BANDS, num_classes=3)\nmodel = model.to(device)\nprint(model)\n\n# %% [code]\n# Setup loss, optimizer and training parameters\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 50\n\n# Define training and evaluation loops\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n    return running_loss / len(loader.dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:56:18.958219Z","iopub.execute_input":"2025-04-15T18:56:18.958615Z","iopub.status.idle":"2025-04-15T18:56:19.008607Z","shell.execute_reply.started":"2025-04-15T18:56:18.958585Z","shell.execute_reply":"2025-04-15T18:56:19.007635Z"}},"outputs":[{"name":"stdout","text":"cuda:1\nSSRN_ViT(\n  (ssrn): SSRN(\n    (conv1): Conv2d(100, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU()\n    (res_block): Sequential(\n      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (vit_head): ViTHead(\n    (patch_embed): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (transformer_encoder): TransformerEncoder(\n      (layers): ModuleList(\n        (0-1): 2 x TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n          )\n          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (classifier): Linear(in_features=256, out_features=3, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\nmodel = SSRN_ViT(in_channels=SELECTED_BANDS, num_classes=3)\nmodel = model.to(device)\nprint(model)\n\n# %% [code]\n# Setup loss, optimizer and training parameters\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 50\n\n# Define training and evaluation loops\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n    return running_loss / len(loader.dataset)\n\ndef eval_epoch(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (preds == labels).sum().item()\n    return running_loss / len(loader.dataset), 100 * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:56:28.588363Z","iopub.execute_input":"2025-04-15T18:56:28.588707Z","iopub.status.idle":"2025-04-15T18:56:28.620620Z","shell.execute_reply.started":"2025-04-15T18:56:28.588681Z","shell.execute_reply":"2025-04-15T18:56:28.619786Z"}},"outputs":[{"name":"stdout","text":"SSRN_ViT(\n  (ssrn): SSRN(\n    (conv1): Conv2d(100, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU()\n    (res_block): Sequential(\n      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (vit_head): ViTHead(\n    (patch_embed): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (transformer_encoder): TransformerEncoder(\n      (layers): ModuleList(\n        (0-1): 2 x TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n          )\n          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (classifier): Linear(in_features=256, out_features=3, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss, val_acc = eval_epoch(model, val_loader, criterion, device)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:56:32.047569Z","iopub.execute_input":"2025-04-15T18:56:32.047896Z","iopub.status.idle":"2025-04-15T18:56:43.308803Z","shell.execute_reply.started":"2025-04-15T18:56:32.047871Z","shell.execute_reply":"2025-04-15T18:56:43.307428Z"}},"outputs":[{"name":"stderr","text":"                                                \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-1849d9587061>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-7cd4b96527bf>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 1 has a total capacity of 14.74 GiB of which 1.20 GiB is free. Process 3981 has 13.54 GiB memory in use. Of the allocated memory 12.17 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.53 GiB. GPU 1 has a total capacity of 14.74 GiB of which 1.20 GiB is free. Process 3981 has 13.54 GiB memory in use. Of the allocated memory 12.17 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"test_loss, test_acc = eval_epoch(model, test_loader, criterion, device)\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n\n# %% [code]\n# Save the trained model\nmodel_save_path = \"/kaggle/working/ssrn_vit.pth\"\ntorch.save(model.state_dict(), model_save_path)\nprint(\"Model saved as\", model_save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:55:53.737379Z","iopub.execute_input":"2025-04-15T18:55:53.737700Z","iopub.status.idle":"2025-04-15T18:55:58.303814Z","shell.execute_reply.started":"2025-04-15T18:55:53.737674Z","shell.execute_reply":"2025-04-15T18:55:58.302428Z"}},"outputs":[{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-2d123ad3805a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# %% [code]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-7cd4b96527bf>\u001b[0m in \u001b[0;36meval_epoch\u001b[0;34m(model, loader, criterion, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}